<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
      <link rel="shortcut icon" href="../img/favicon.ico" />
    <title>User Guide - A2RCHI Docs</title>
    <link rel="stylesheet" href="../css/theme.css" />
    <link rel="stylesheet" href="../css/theme_extra.css" />
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/styles/github.min.css" />
    
      <script>
        // Current page data
        var mkdocs_page_name = "User Guide";
        var mkdocs_page_input_path = "user_guide.md";
        var mkdocs_page_url = null;
      </script>
    
    <!--[if lt IE 9]>
      <script src="../js/html5shiv.min.js"></script>
    <![endif]-->
      <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/highlight.min.js"></script>
      <script>hljs.highlightAll();</script> 
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
    <div class="wy-side-scroll">
      <div class="wy-side-nav-search">
          <a href=".." class="icon icon-home"> A2RCHI Docs
        </a>
        <div class="version">
          1.2.4
        </div><div role="search">
  <form id ="rtd-search-form" class="wy-form" action="../search.html" method="get">
      <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" title="Type search term here" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="..">A2RCHI</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../install/">Install</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../quickstart/">Quickstart</a>
                </li>
              </ul>
              <ul class="current">
                <li class="toctree-l1 current"><a class="reference internal current" href="#">User Guide</a>
    <ul class="current">
    <li class="toctree-l2"><a class="reference internal" href="#overview">Overview</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#optional-command-line-options">Optional command line options</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#data-sources">Data Sources</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#web-link-lists">Web Link Lists</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="#configuration">Configuration</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#secrets">Secrets</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#running">Running</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#git-scraping">Git scraping</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="#configuration_1">Configuration</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#secrets_1">Secrets</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#running_1">Running</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#jira">JIRA</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="#configuration_2">Configuration</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#secrets_2">Secrets</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#running_2">Running</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#adding-documents-and-the-uploader-interface">Adding Documents and the Uploader Interface</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="#adding-documents">Adding Documents</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#manual-uploader">Manual Uploader</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#directly-copying-files-to-the-container">Directly copying files to the container</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#redmine">Redmine</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="#configuration_3">Configuration</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#secrets_3">Secrets</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#running_3">Running</a>
    </li>
        </ul>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#interfacesservices">Interfaces/Services</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#piazza-interface">Piazza Interface</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="#configuration_4">Configuration</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#secrets_4">Secrets</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#running_4">Running</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#redminemailbox-interface">Redmine/Mailbox Interface</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="#configuration_5">Configuration</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#secrets_5">Secrets</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#running_5">Running</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#mattermost-interface">Mattermost Interface</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="#configuration_6">Configuration</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#secrets_6">Secrets</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#running_6">Running</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#grafana-interface">Grafana Interface</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="#configuration_7">Configuration</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#secrets_7">Secrets</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#running_7">Running</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#grader-interface">Grader Interface</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="#requirements">Requirements</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#secrets_8">Secrets</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#configuration_8">Configuration</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#running_8">Running</a>
    </li>
        </ul>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#models">Models</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#local-models">Local Models</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#models-via-apis">Models via APIs</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#ollama">Ollama</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#vector-store">Vector Store</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#configuration_9">Configuration</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="#core-settings">Core Settings</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#distance-metrics">Distance Metrics</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#embedding-models">Embedding Models</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="#openai-embeddings">OpenAI Embeddings</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#huggingface-embeddings">HuggingFace Embeddings</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#supported-document-formats">Supported Document Formats</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#document-synchronization">Document Synchronization</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#hybrid-search">Hybrid Search</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#stemming">Stemming</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#chromadb-backend">ChromaDB Backend</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="#local-persistent">Local (Persistent)</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#remote-http-client">Remote (HTTP Client)</a>
    </li>
        </ul>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#benchmarking">Benchmarking</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#preparing-the-queries-file">Preparing the queries file</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#configuration_10">Configuration</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#running_9">Running</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#additional-options">Additional options</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#results">Results</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#other">Other</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#add-chromadb-document-management-api-endpoints">Add ChromaDB Document Management API Endpoints</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="#debugging-chromadb-endpoints">Debugging ChromaDB endpoints</a>
    </li>
        </ul>
    </li>
        </ul>
    </li>
    </ul>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../advanced_setup_deploy/">Advanced Setup and Deployment</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../api_reference/">API Reference</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../developer_guide/">Developer Guide</a>
                </li>
              </ul>
      </div>
    </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">
      <nav class="wy-nav-top" role="navigation" aria-label="Mobile navigation menu">
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="..">A2RCHI Docs</a>
        
      </nav>
      <div class="wy-nav-content">
        <div class="rst-content"><div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href=".." class="icon icon-home" aria-label="Docs"></a></li>
      <li class="breadcrumb-item active">User Guide</li>
    <li class="wy-breadcrumbs-aside">
          <a href="https://github.com/mit-submit/A2rchi/edit/master/docs/user_guide.md" class="icon icon-github"> Edit on GitHub</a>
    </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
            <div class="section" itemprop="articleBody">
              
                <h1 id="user-guide">User Guide</h1>
<h2 id="overview">Overview</h2>
<p>A2RCHI supports various <strong>data sources</strong> as easy ways to ingest your data into the vector store databased used for document retrieval. These include:</p>
<ul>
<li><strong>Links lists (even behind SSO)</strong>: automatically scrape and ingest documents from a list of URLs</li>
<li><strong>Git scraping</strong>: git mkdocs repositories</li>
<li><strong>Ticketing systems</strong>: JIRA, Redmine, Piazza</li>
<li><strong>Local documents</strong></li>
</ul>
<p>Additionally, A2RCHI supports various <strong>interfaces/services</strong>, which are applications that interact with the RAG system. These include:</p>
<ul>
<li><strong>Chat interface</strong>: a web-based chat application</li>
<li><strong>Piazza integration</strong>: read posts from Piazza and post draft responses to a Slack channel</li>
<li><strong>Cleo/Redmine integration</strong>: read emails and create tickets in Redmine</li>
<li><strong>Mattermost integration</strong>: read posts from Mattermost and post draft responses to a Mattermost channel</li>
<li><strong>Grafana monitoring dashboard</strong>: monitor system and LLM performance metrics</li>
<li><strong>Document uploader</strong>: web interface for uploading and managing documents</li>
<li><strong>Grader</strong>: automated grading service for assignments with web interface</li>
</ul>
<p>Both data sources and interfaces/services are enabled via flags to the <code>a2rchi create</code> command,</p>
<pre><code class="language-bash">a2rchi create [...] --services=chatbot,piazza,... --sources jira,redmine,...
</code></pre>
<p>The parameters of the services and sources are configured via the configuration file. See below for more details.</p>
<p>We support various <strong>pipelines</strong> which are pre-defined sequences of operations that process user inputs and generate responses.
Each service may support a given pipeline.
See the <code>Services</code> and <code>Pipelines</code> sections below for more details.
For each pipeline, you can use different models, retrievers, and prompts for different steps of the pipeline.
We support various <strong>models</strong> for both embeddings and LLMs, which can be run locally or accessed via APIs.
See the <code>Models</code> section below for more details.
Both pipelines and models are configured via the configuration file.</p>
<p>Finally, we support various <strong>retrievers</strong> and <strong>embedding techniques</strong> for document retrieval.
These are configured via the configuration file.
See the <code>Vector Store</code> section below for more details.</p>
<h3 id="optional-command-line-options">Optional command line options</h3>
<p>In addition to the required <code>--name</code>, <code>--config/--config-dir</code>, <code>--env-file</code>, and <code>--services</code> arguments, the <code>a2rchi create</code> command accepts several useful flags:</p>
<ol>
<li><strong><code>--podman</code></strong>: Run the deployment with Podman instead of Docker.</li>
<li><strong><code>--sources</code> / <code>-src</code></strong>: Enable additional ingestion sources (<code>git</code>, <code>sso</code>, <code>jira</code>, <code>redmine</code>, ...). Provide a comma-separated list.</li>
<li><strong><code>--gpu-ids</code></strong>: Mount specific GPUs (<code>--gpu-ids all</code> or <code>--gpu-ids 0,1</code>). The legacy <code>--gpu</code> flag still works but maps to <code>all</code>.</li>
<li><strong><code>--tag</code></strong>: Override the local image tag (defaults to <code>2000</code>). Handy when building multiple configurations side-by-side.</li>
<li><strong><code>--hostmode</code></strong>: Use host networking for all services.</li>
<li><strong><code>--verbosity</code> / <code>-v</code></strong>: Control CLI logging level (0 = quiet, 4 = debug).</li>
<li><strong><code>--force</code></strong> / <strong><code>--dry-run</code></strong>: Force recreation of an existing deployment and/or show what would happen without actually deploying.</li>
</ol>
<p>You can inspect the available services and sources, together with descriptions, using <code>a2rchi list-services</code>.</p>
<blockquote>
<p><strong>GPU helpers</strong></p>
<p>GPU access requires the NVIDIA drivers plus the NVIDIA Container Toolkit. After installing the toolkit, generate CDI entries (for Podman) with <code>sudo nvidia-ctk cdi generate --output=/etc/cdi/nvidia.yaml</code> and confirm with <code>nvidia-ctk cdi list</code>. Docker users should run <code>sudo nvidia-ctk runtime configure --runtime=docker</code>.</p>
</blockquote>
<hr />
<h2 id="data-sources">Data Sources</h2>
<p>These are the different ways to ingest data into the vector store used for document retrieval.</p>
<h3 id="web-link-lists">Web Link Lists</h3>
<p>A web link list is a simple text file containing a list of URLs, one per line.
A2RCHI will fetch the content from each URL and add it to the vector store, using the <code>Scraper</code> class.</p>
<h4 id="configuration">Configuration</h4>
<p>You can define which lists of links A2RCHI will ingest in the configuration file as follows:</p>
<pre><code class="language-yaml">data_manager:
  sources:
    links:
      input_lists:  # REQUIRED
        - miscellanea.list  # list of websites with relevant info
        - [...other lists...]
</code></pre>
<p>Each list should be a simple text file containing one URL per line, e.g.,</p>
<pre><code>https://example.com/page1
https://example.com/page2
[...]
</code></pre>
<p>In the case that some of the links are behind a Single Sign-On (SSO) system, enable the SSO source in your configuration and specify the collector class:</p>
<pre><code class="language-yaml">data_manager:
  sources:
    sso:
      enabled: true
      sso_class: CERNSSOScraper  # or whichever class is appropriate
      sso_class_map:
        CERNSSOScraper:
          kwargs:
            headless: true
            max_depth: 2
</code></pre>
<p>Then, run <code>a2rchi create ... --sources sso</code> to activate the SSO collector.</p>
<p>You can customise the HTTP scraper behaviour (for example, to avoid SSL verification warnings):</p>
<pre><code class="language-yaml">data_manager:
  sources:
    links:
      scraper:
        reset_data: true
        verify_urls: false
        enable_warnings: false
</code></pre>
<h4 id="secrets">Secrets</h4>
<p>If you are using SSO, depending on the class, you may need to provide your login credentials in a secrets file as follows:</p>
<pre><code class="language-bash">SSO_USERNAME=username
SSO_PASSWORD=password
</code></pre>
<p>Then, make sure that the links you provide in the <code>.list</code> file(s) start with <code>sso-</code>, e.g.,</p>
<pre><code>sso-https://example.com/protected/page
</code></pre>
<h4 id="running">Running</h4>
<p>Link scraping is automatically enabled in A2RCHI, you don't need to add any arguments to the <code>create</code> command unless the links are sso protected.</p>
<hr />
<h3 id="git-scraping">Git scraping</h3>
<p>In some cases, the RAG input may be documentations based on MKDocs git repositories.
Instead of scraping these sites as regular HTML sites you can obtain the relevant content using the <code>GitScraper</code> class.</p>
<h4 id="configuration_1">Configuration</h4>
<p>To configure it, enable the git source in the configuration file:</p>
<pre><code class="language-yaml">data_manager:
  sources:
    git:
      enabled: true
</code></pre>
<p>In the input lists, make sure to prepend <code>git-</code> to the URL of the <strong>repositories</strong> you are interested in scraping.</p>
<pre><code>git-https://github.com/example/mkdocs/documentation.git
</code></pre>
<h4 id="secrets_1">Secrets</h4>
<p>You will need to provide a git username and token in the secrets file,</p>
<pre><code class="language-bash">GIT_USERNAME=your_username
GIT_TOKEN=your_token
</code></pre>
<h4 id="running_1">Running</h4>
<p>Enable the git source during deployment with <code>--sources git</code>.</p>
<hr />
<h3 id="jira">JIRA</h3>
<p>The JIRA integration allows A2RCHI to fetch issues and comments from specified JIRA projects and add them to the vector store, using the <code>JiraClient</code> class.</p>
<h4 id="configuration_2">Configuration</h4>
<p>Select which projects to scrape in the configuration file:</p>
<pre><code class="language-yaml">data_manager:
  sources:
    jira:
      url: https://jira.example.com
      projects:
        - PROJECT_KEY
      anonymize_data: true
</code></pre>
<p>You can further customise anonymisation via the global anonymiser settings.</p>
<pre><code class="language-yaml">data_manager:
  utils:
    anonymizer:
      nlp_model: en_core_web_sm
      excluded_words:
        - Example
      greeting_patterns:
        - '^(hi|hello|hey|greetings|dear)\b'
      signoff_patterns:
        - '\b(regards|sincerely|best regards|cheers|thank you)\b'
      email_pattern: '[\w\.-]+@[\w\.-]+\.\w+'
      username_pattern: '\[~[^\]]+\]'
</code></pre>
<p>The anonymizer will remove names, emails, usernames, greetings, signoffs, and any other words you specify from the fetched data.
This is useful if you want to avoid having personal information in the vector store.</p>
<h4 id="secrets_2">Secrets</h4>
<p>A personal access token (PAT) is required to authenticate and authorize with JIRA.
Add <code>JIRA_PAT=&lt;token&gt;</code> to your <code>.env</code> file before deploying with <code>--sources jira</code>.</p>
<h4 id="running_2">Running</h4>
<p>Enable the source at deploy time with:</p>
<pre><code class="language-bash">a2rchi create [...] --services=chatbot --sources jira
</code></pre>
<hr />
<h3 id="adding-documents-and-the-uploader-interface">Adding Documents and the Uploader Interface</h3>
<h4 id="adding-documents">Adding Documents</h4>
<p>There are two main ways to add documents to A2RCHI's vector database. They are:</p>
<ul>
<li>Manually adding files while the service is running via the uploader GUI</li>
<li>Directly copying files into the container</li>
</ul>
<p>These methods are outlined below.</p>
<h4 id="manual-uploader">Manual Uploader</h4>
<p>In order to upload documents while A2RCHI is running via an easily accessible GUI, enable the uploader service when creating the deployment:</p>
<pre><code class="language-bash">a2rchi create [...] --services=chatbot,uploader
</code></pre>
<p>The exact port may vary based on configuration (default external port is <code>5003</code>).
A quick <code>podman ps</code> or <code>docker ps</code> will show which port is exposed.</p>
<p>In order to access the manager, you must first create an admin account. Grab the container ID with <code>podman ps</code>/<code>docker ps</code> and then enter the container:</p>
<pre><code>docker exec -it &lt;CONTAINER-ID&gt; bash
</code></pre>
<p>Run the bundled helper:</p>
<pre><code>python -u src/bin/service_create_account.py
</code></pre>
<p>from the <code>/root/A2RCHI</code> directory inside the container. This script will guide you through creating an account; never reuse sensitive passwords here.</p>
<p>Once you have created an account, visit the outgoing port of the data manager docker service and then log in.
The GUI will then allow you to upload documents while A2RCHI is still running. Note that it may take a few minutes for all the documents to upload.</p>
<h4 id="directly-copying-files-to-the-container">Directly copying files to the container</h4>
<p>The documents used for RAG live in the chat container at <code>/root/data/&lt;directory&gt;/&lt;files&gt;</code>. Thus, in a pinch, you can <code>docker/podman cp</code> a file at this directory level, e.g., <code>podman/docker cp myfile.pdf &lt;container name or ID&gt;:/root/data/&lt;new_dir&gt;/</code>. If you need to make a new directory in the container, you can do <code>podman exec -it &lt;container name or ID&gt; mkdir /root/data/&lt;new_dir&gt;</code>.</p>
<hr />
<h3 id="redmine">Redmine</h3>
<p>Use the Redmine source to ingest solved tickets (question/answer pairs) into the vector store.</p>
<h4 id="configuration_3">Configuration</h4>
<pre><code class="language-yaml">data_manager:
  sources:
    redmine:
      url: https://redmine.example.com
      project: my-project
      anonymize_data: true
</code></pre>
<h4 id="secrets_3">Secrets</h4>
<p>Add the following to your <code>.env</code> file:</p>
<pre><code class="language-bash">REDMINE_USER=...
REDMINE_PW=...
</code></pre>
<h4 id="running_3">Running</h4>
<p>Enable the source at deploy time with:</p>
<pre><code class="language-bash">a2rchi create [...] --services=chatbot --sources redmine
</code></pre>
<blockquote>
<p>To automate email replies, also enable the <code>redmine-mailer</code> service (see the Services section below).</p>
</blockquote>
<hr />
<h2 id="interfacesservices">Interfaces/Services</h2>
<p>These are the different apps that A2RCHI supports, which allow you to interact with the AI pipelines.</p>
<h3 id="piazza-interface">Piazza Interface</h3>
<p>Set up A2RCHI to read posts from your Piazza forum and post draft responses to a specified Slack channel. To do this, a Piazza login (email and password) is required, plus the network ID of your Piazza channel, and lastly, a Webhook for the slack channel A2RCHI will post to. See below for a step-by-step description of this.</p>
<ol>
<li>Go to <a href="https://api.slack.com/apps">https://api.slack.com/apps</a> and sign in to workspace where you will eventually want A2RCHI to post to (note doing this in a business workspace like the MIT one will require approval of the app/bot).</li>
<li>Click 'Create New App', and then 'From scratch'. Name your app and again select the correct workspace. Then hit 'Create App'</li>
<li>Now you have your app, and there are a few things to configure before you can launch A2RCHI:</li>
<li>Go to Incoming Webhooks under Features, and toggle it on.</li>
<li>Click 'Add New Webhook', and select the channel you want A2RCHI to post to.</li>
<li>Now, copy the 'Webhook URL' and paste it into the secrets file, and handle it like any other secret!</li>
</ol>
<h4 id="configuration_4">Configuration</h4>
<p>Beyond standard required configuration fields, the network ID of the Piazza channel is required (see below for an example config). You can get the network ID by simply navigating to the class homepage, and grabbing the sequence that follows 'https://piazza.com/class/'. For example, the 8.01 Fall 2024 homepage is: 'https://piazza.com/class/m0g3v0ahsqm2lg'. The network ID is thus 'm0g3v0ahsqm2lg'.</p>
<p>Example minimal config for the Piazza interface:</p>
<pre><code class="language-yaml">name: bare_minimum_configuration #REQUIRED

data_manager:
  sources:
    links:
      input_lists:
        - class_info.list # class info links

a2rchi:
  [... a2rchi config ...]

services:
  piazza:
    network_id: &lt;your Piazza network ID here&gt; # REQUIRED
  chat_app:
    trained_on: &quot;Your class materials&quot; # REQUIRED
</code></pre>
<h4 id="secrets_4">Secrets</h4>
<p>The necessary secrets for deploying the Piazza service are the following:</p>
<pre><code class="language-bash">PIAZZA_EMAIL=...
PIAZZA_PASSWORD=...
SLACK_WEBHOOK=...
</code></pre>
<p>The Slack webhook secret is described above. The Piazza email and password should be those of one of the class instructors. Remember to put this information in files named following what is written above.</p>
<h4 id="running_4">Running</h4>
<p>To run the Piazza service, simply add the piazza flag. For example:</p>
<pre><code class="language-bash">a2rchi create [...] --services=chatbot,piazza
</code></pre>
<hr />
<h3 id="redminemailbox-interface">Redmine/Mailbox Interface</h3>
<p>A2RCHI will read all new tickets in a Redmine project, and draft a response as a comment to the ticket.
Once the ticket is updated to the "Resolved" status by an admin, A2RCHI will send the response as an email to the user who opened the ticket.
The admin can modify A2RCHI's response before sending it out.</p>
<h4 id="configuration_5">Configuration</h4>
<pre><code class="language-yaml">services:
  redmine_mailbox:
    url: https://redmine.example.com
    project: my-project
    redmine_update_time: 10
    mailbox_update_time: 10
    answer_tag: &quot;-- A2RCHI -- Resolving email was sent&quot;
</code></pre>
<h4 id="secrets_5">Secrets</h4>
<p>Add the following secrets to your <code>.env</code> file:</p>
<pre><code class="language-bash">IMAP_USER=...
IMAP_PW=...
REDMINE_USER=...
REDMINE_PW=...
SENDER_SERVER=...
SENDER_PORT=587
SENDER_REPLYTO=...
SENDER_USER=...
SENDER_PW=...
</code></pre>
<h4 id="running_5">Running</h4>
<pre><code class="language-bash">a2rchi create [...] --services=chatbot,redmine-mailer
</code></pre>
<hr />
<h3 id="mattermost-interface">Mattermost Interface</h3>
<p>Set up A2RCHI to read posts from your Mattermost forum and post draft responses to a specified Mattermost channel.</p>
<h4 id="configuration_6">Configuration</h4>
<pre><code class="language-yaml">services:
  mattermost:
    update_time: 60
</code></pre>
<h4 id="secrets_6">Secrets</h4>
<p>You need to specify a webhook, access token, and channel identifiers:</p>
<pre><code class="language-bash">MATTERMOST_WEBHOOK=...
MATTERMOST_PAK=...
MATTERMOST_CHANNEL_ID_READ=...
MATTERMOST_CHANNEL_ID_WRITE=...
</code></pre>
<h4 id="running_6">Running</h4>
<p>To run the Mattermost service, include it when selecting services. For example:</p>
<pre><code class="language-bash">a2rchi create [...] --services=chatbot,mattermost
</code></pre>
<hr />
<h3 id="grafana-interface">Grafana Interface</h3>
<p>Monitor the performance of your A2RCHI instance with the Grafana interface. This service provides a web-based dashboard to visualize various metrics related to system performance, LLM usage, and more.</p>
<blockquote>
<p>Note, if you are deploying a version of A2RCHI you have already used (i.e., you haven't removed the images/volumes for a given <code>--name</code>), the postgres will have already been created without the Grafana user created, and it will not work, so make sure to deploy a fresh instance.</p>
</blockquote>
<h4 id="configuration_7">Configuration</h4>
<pre><code class="language-yaml">services:
  grafana:
    external_port: 3000
</code></pre>
<h4 id="secrets_7">Secrets</h4>
<p>Grafana shares the Postgres database with other services, so you need both the database password and a Grafana-specific password:</p>
<pre><code class="language-bash">PG_PASSWORD=&lt;your_database_password&gt;
GRAFANA_PG_PASSWORD=&lt;grafana_db_password&gt;
</code></pre>
<h4 id="running_7">Running</h4>
<p>Deploy Grafana alongside your other services:</p>
<pre><code class="language-bash">a2rchi create [...] --services=chatbot,grafana
</code></pre>
<p>and you should see something like this</p>
<pre><code>CONTAINER ID  IMAGE                                     COMMAND               CREATED        STATUS                  PORTS                             NAMES
d27482864238  localhost/chromadb-gtesting2:2000         uvicorn chromadb....  9 minutes ago  Up 9 minutes (healthy)  0.0.0.0:8000-&gt;8000/tcp, 8000/tcp  chromadb-gtesting2
87f1c7289d29  docker.io/library/postgres:16             postgres              9 minutes ago  Up 9 minutes (healthy)  5432/tcp                          postgres-gtesting2
40130e8e23de  docker.io/library/grafana-gtesting2:2000                        9 minutes ago  Up 9 minutes            0.0.0.0:3000-&gt;3000/tcp, 3000/tcp  grafana-gtesting2
d6ce8a149439  localhost/chat-gtesting2:2000             python -u a2rchi/...  9 minutes ago  Up 9 minutes            0.0.0.0:7861-&gt;7861/tcp            chat-gtesting2
</code></pre>
<p>where the grafana interface is accessible at <code>your-hostname:3000</code>. To change the external port from <code>3000</code>, you can do this in the config at <code>services.grafana.external_port</code>. The default login and password are both "admin", which you will be prompted to change should you want to after first logging in. Navigate to the A2RCHI dashboard from the home page by going to the menu &gt; Dashboards &gt; A2RCHI &gt; A2RCHI Usage. Note, <code>your-hostname</code> here is the just name of the machine. Grafana uses its default configuration which is <code>localhost</code> but unlike the chat interface, there are no APIs where we template with a selected hostname, so the container networking handles this nicely.</p>
<blockquote>
<p>Pro tip: once at the web interface, for the "Recent Conversation Messages (Clean Text + Link)" panel, click the three little dots in the top right hand corner of the panel, click "Edit", and on the right, go to e.g., "Override 4" (should have Fields with name: clean text, also Override 7 for context column) and override property "Cell options &gt; Cell value inspect". This will allow you to expand the text boxes with messages longer than can fit. Make sure you click apply to keep the changes.</p>
<p>Pro tip 2: If you want to download all of the information from any panel as a CSV, go to the same three dots and click "Inspect", and you should see the option.</p>
</blockquote>
<hr />
<h3 id="grader-interface">Grader Interface</h3>
<p>Interface to launch a website which for a provided solution and rubric (and a couple of other things detailed below), will grade scanned images of a handwritten solution for the specified problem(s).</p>
<blockquote>
<p>Nota bene: this is not yet fully generalized and "service" ready, but instead for testing grading pipelines and a base off of which to build a potential grading app.</p>
</blockquote>
<h4 id="requirements">Requirements</h4>
<p>To launch the service the following files are required:</p>
<ul>
<li><code>users.csv</code>. This file is .csv file that contains two columns: "MIT email" and "Unique code", e.g.:</li>
</ul>
<pre><code>MIT email,Unique code
username@mit.edu,222
</code></pre>
<p>For now, the system requires the emails to be in the MIT domain, namely, contain "@mit.edu". TODO: make this an argument that is passed (e.g., school/email domain)</p>
<ul>
<li><code>solution_with_rubric_*.txt</code>. These are .txt files that contain the problem solution followed by the rubric. The naming of the files should follow exactly, where the <code>*</code> is the problem number. There should be one of these files for every problem you want the app to be able to grade. The top of the file should be the problem name with a line of dashes ("-") below, e.g.:</li>
</ul>
<pre><code>Anti-Helmholtz Coils
---------------------------------------------------
</code></pre>
<p>These files should live in a directory which you will pass to the config, and A2RCHI will handle the rest.</p>
<ul>
<li><code>admin_password.txt</code>. This file will be passed as a secret and be the admin code to login in to the page where you can reset attempts for students.</li>
</ul>
<h4 id="secrets_8">Secrets</h4>
<p>The only grading specific secret is the admin password, which like shown above, should be put in the following file</p>
<pre><code class="language-bash">ADMIN_PASSWORD=your_password
</code></pre>
<p>Then it behaves like any other secret.</p>
<h4 id="configuration_8">Configuration</h4>
<p>The required fields in the configuration file are different from the rest of the A2RCHI services. Below is an example:</p>
<pre><code class="language-yaml">name: grading_test # REQUIRED

a2rchi:
  pipelines:
    - GradingPipeline
  pipeline_map:
    GradingPipeline:
      prompts:
        required:
          final_grade_prompt: final_grade.prompt
      models:
        required:
          final_grade_model: OllamaInterface
    ImageProcessingPipeline:
      prompts:
        required:
          image_processing_prompt: image_processing.prompt
      models:
        required:
          image_processing_model: OllamaInterface

services:
  chat_app:
    trained_on: &quot;rubrics, class info, etc.&quot; # REQUIRED
  grader_app:
    num_problems: 1 # REQUIRED
    local_rubric_dir: ~/grading/my_rubrics # REQUIRED
    local_users_csv_dir: ~/grading/logins # REQUIRED

data_manager:
  [...]
</code></pre>
<ol>
<li><code>name</code> -- The name of your configuration (required).</li>
<li><code>a2rchi.pipelines</code> -- List of pipelines to use (e.g., <code>GradingPipeline</code>, <code>ImageProcessingPipeline</code>).</li>
<li><code>a2rchi.pipeline_map</code> -- Mapping of pipelines to their required prompts and models.</li>
<li><code>a2rchi.pipeline_map.GradingPipeline.prompts.required.final_grade_prompt</code> -- Path to the grading prompt file for evaluating student solutions.</li>
<li><code>a2rchi.pipeline_map.GradingPipeline.models.required.final_grade_model</code> -- Model class for grading (e.g., <code>OllamaInterface</code>, <code>HuggingFaceOpenLLM</code>).</li>
<li><code>a2rchi.pipeline_map.ImageProcessingPipeline.prompts.required.image_processing_prompt</code> -- Path to the prompt file for image processing.</li>
<li><code>a2rchi.pipeline_map.ImageProcessingPipeline.models.required.image_processing_model</code> -- Model class for image processing (e.g., <code>OllamaInterface</code>, <code>HuggingFaceImageLLM</code>).</li>
<li><code>services.chat_app.trained_on</code> -- A brief description of the data or materials A2RCHI is trained on (required).</li>
<li><code>services.grader_app.num_problems</code> -- Number of problems the grading service should expect (must match the number of rubric files).</li>
<li><code>services.grader_app.local_rubric_dir</code> -- Directory containing the <code>solution_with_rubric_*.txt</code> files.</li>
<li><code>services.grader_app.local_users_csv_dir</code> -- Directory containing the <code>users.csv</code> file.</li>
</ol>
<h4 id="running_8">Running</h4>
<pre><code class="language-bash">a2rchi create [...] --services=grader
</code></pre>
<hr />
<h2 id="models">Models</h2>
<p>Models are either:</p>
<ol>
<li>Hosted locally, either via VLLM or HuggingFace transformers.</li>
<li>Accessed via an API, e.g., OpenAI, Anthropic, etc.</li>
<li>Accessed via an Ollama server instance.</li>
</ol>
<h3 id="local-models">Local Models</h3>
<p>To use a local model, specify one of the local model classes in <code>models.py</code>:</p>
<ul>
<li><code>HuggingFaceOpenLLM</code></li>
<li><code>HuggingFaceImageLLM</code></li>
<li><code>VLLM</code></li>
</ul>
<h3 id="models-via-apis">Models via APIs</h3>
<p>We support the following model classes in <code>models.py</code> for models accessed via APIs:</p>
<ul>
<li><code>OpenAILLM</code></li>
<li><code>AnthropicLLM</code></li>
</ul>
<h3 id="ollama">Ollama</h3>
<p>In order to use an Ollama server instance for the chatbot, it is possible to specify <code>OllamaInterface</code> for the model name. To then correctly use models on the Ollama server, in the keyword args, specify both the url of the server and the name of a model hosted on the server.</p>
<pre><code class="language-yaml">a2rchi:
  model_class_map:
    OllamaInterface:
      kwargs:
        base_model: &quot;gemma3&quot; # example
        url: &quot;url-for-server&quot;

</code></pre>
<p>In this case, the <code>gemma3</code> model is hosted on the Ollama server at <code>url-for-server</code>. You can check which models are hosted on your server by going to <code>url-for-server/models</code>.</p>
<hr />
<h2 id="vector-store">Vector Store</h2>
<p>The vector store is a database that stores document embeddings, enabling semantic and/or lexical search over your knowledge base. A2RCHI uses ChromaDB as the vector store backend to index and retrieve relevant documents based on similarity to user queries.</p>
<h3 id="configuration_9">Configuration</h3>
<p>Vector store settings are configured under the <code>data_manager</code> section:</p>
<pre><code class="language-yaml">data_manager:
  collection_name: default_collection
  embedding_name: OpenAIEmbeddings
  chunk_size: 1000
  chunk_overlap: 0
  reset_collection: true
  num_documents_to_retrieve: 5
  distance_metric: cosine
</code></pre>
<h4 id="core-settings">Core Settings</h4>
<ul>
<li><strong><code>collection_name</code></strong>: Name of the ChromaDB collection. Default: <code>default_collection</code></li>
<li><strong><code>chunk_size</code></strong>: Maximum size of text chunks (in characters) when splitting documents. Default: <code>1000</code></li>
<li><strong><code>chunk_overlap</code></strong>: Number of overlapping characters between consecutive chunks. Default: <code>0</code></li>
<li><strong><code>reset_collection</code></strong>: If <code>true</code>, deletes and recreates the collection on startup. Default: <code>true</code></li>
<li><strong><code>num_documents_to_retrieve</code></strong>: Number of relevant document chunks to retrieve for each query. Default: <code>5</code></li>
</ul>
<h4 id="distance-metrics">Distance Metrics</h4>
<p>The <code>distance_metric</code> determines how similarity is calculated between embeddings:</p>
<ul>
<li><strong><code>cosine</code></strong>: Cosine similarity (default) - measures the angle between vectors</li>
<li><strong><code>l2</code></strong>: Euclidean distance - measures straight-line distance</li>
<li><strong><code>ip</code></strong>: Inner product - measures dot product similarity</li>
</ul>
<pre><code class="language-yaml">data_manager:
  distance_metric: cosine  # Options: cosine, l2, ip
</code></pre>
<h3 id="embedding-models">Embedding Models</h3>
<p>Embeddings convert text into numerical vectors. A2RCHI supports multiple embedding providers:</p>
<h4 id="openai-embeddings">OpenAI Embeddings</h4>
<pre><code class="language-yaml">data_manager:
  embedding_name: OpenAIEmbeddings
  embedding_class_map:
    OpenAIEmbeddings:
      class: OpenAIEmbeddings
      kwargs:
        model: text-embedding-3-small
      similarity_score_reference: 10
</code></pre>
<h4 id="huggingface-embeddings">HuggingFace Embeddings</h4>
<pre><code class="language-yaml">data_manager:
  embedding_name: HuggingFaceEmbeddings
  embedding_class_map:
    HuggingFaceEmbeddings:
      class: HuggingFaceEmbeddings
      kwargs:
        model_name: sentence-transformers/all-MiniLM-L6-v2
        model_kwargs:
          device: cpu
        encode_kwargs:
          normalize_embeddings: true
      similarity_score_reference: 10
      query_embedding_instructions: null
</code></pre>
<h3 id="supported-document-formats">Supported Document Formats</h3>
<p>The vector store can process the following file types:</p>
<ul>
<li><strong>Text files</strong>: <code>.txt</code>, <code>.C</code></li>
<li><strong>Markdown</strong>: <code>.md</code></li>
<li><strong>Python</strong>: <code>.py</code></li>
<li><strong>HTML</strong>: <code>.html</code></li>
<li><strong>PDF</strong>: <code>.pdf</code></li>
</ul>
<p>Documents are automatically loaded with the appropriate parser based on file extension.</p>
<h3 id="document-synchronization">Document Synchronization</h3>
<p>A2RCHI automatically synchronizes your data directory with the vector store:</p>
<ol>
<li><strong>Adding documents</strong>: New files in the data directory are automatically chunked, embedded, and added to the collection</li>
<li><strong>Removing documents</strong>: Files deleted from the data directory are removed from the collection</li>
<li><strong>Source tracking</strong>: Each ingested artifact is recorded in the unified <code>index.yaml</code> file as <code>&lt;resource-hash&gt;: &lt;relative file path&gt;</code> inside the data directory</li>
</ol>
<h3 id="hybrid-search">Hybrid Search</h3>
<p>Combine semantic search with keyword-based BM25 search for improved retrieval:</p>
<pre><code class="language-yaml">data_manager:
  use_hybrid_search: true
  bm25_weight: 0.6
  semantic_weight: 0.4
  bm25:
    k1: 0.5
    b: 0.75
</code></pre>
<ul>
<li><strong><code>use_hybrid_search</code></strong>: Enable hybrid search combining BM25 and semantic similarity. Default: <code>false</code></li>
<li><strong><code>bm25_weight</code></strong>: Weight for BM25 keyword scores. Default: <code>0.6</code></li>
<li><strong><code>semantic_weight</code></strong>: Weight for semantic similarity scores. Default: <code>0.4</code></li>
<li><strong><code>bm25.k1</code></strong>: BM25 term frequency saturation parameter. Default: <code>0.5</code></li>
<li><strong><code>bm25.b</code></strong>: BM25 document length normalization parameter. Default: <code>0.75</code></li>
</ul>
<h3 id="stemming">Stemming</h3>
<p>By specifying the stemming option within your configuration, stemming functionality for the documents in A2RCHI will be enabled. By doing so, documents inserted into the retrieval pipeline, as well as the query that is matched with them, will be stemmed and simplified for faster and more accurate lookup.</p>
<pre><code class="language-yaml">data_manager:
  stemming:
    enabled: true
</code></pre>
<p>When enabled, both documents and queries are processed using the Porter Stemmer algorithm to reduce words to their root forms (e.g., "running" â†’ "run"), improving matching accuracy.</p>
<h3 id="chromadb-backend">ChromaDB Backend</h3>
<p>A2RCHI supports both local and remote ChromaDB instances:</p>
<h4 id="local-persistent">Local (Persistent)</h4>
<pre><code class="language-yaml">services:
  chromadb:
    local_vstore_path: /path/to/vectorstore
</code></pre>
<h4 id="remote-http-client">Remote (HTTP Client)</h4>
<pre><code class="language-yaml">services:
  chromadb:
    use_HTTP_chromadb_client: true
    chromadb_host: localhost
    chromadb_port: 8000
</code></pre>
<hr />
<h2 id="benchmarking">Benchmarking</h2>
<p>A2RCHI has benchmarking functionality provided by the <code>evaluate</code> CLI command. We currently support two modes:</p>
<ol>
<li><code>SOURCES</code>: given a user question and a list of correct sources, check if the retrieved documents contain any of the correct sources.</li>
<li><code>RAGAS</code>: use the Ragas RAG evaluator module to return numerical values judging by 4 of their provided metrics the quality of the answer: <code>answer_relevancy</code>, <code>faithfulness</code>, <code>context precision</code>, and <code>context relevancy</code>.</li>
</ol>
<h3 id="preparing-the-queries-file">Preparing the queries file</h3>
<p>Provide your list of questions, answers, and relevant sources in JSON format as follows:</p>
<pre><code class="language-json">[
  {
    &quot;question&quot;: &quot;&quot;,
    &quot;sources&quot;: [...],
    &quot;answer&quot;: &quot;&quot;
    // (optional)
    &quot;sources_match_field&quot;: [...]
  },
  ...
]
</code></pre>
<p>Explanation of fields:
- <code>question</code>: The question to be answered by the A2RCHI instance.
- <code>sources</code>: A list of sources (e.g., URLs, ticket IDs) that contain the answer. They are identified via the <code>sources_match_field</code>, which must be one of the metadata fields of the documents in your vector store.
- <code>answer</code>: The expected answer to the question, used for evaluation.
- <code>sources_match_field</code> (optional): A list of metadata fields to match the sources against (e.g., <code>url</code>, <code>ticket_id</code>). If not provided, defaults to what is in the configuration file under <code>data_manager:services:benchmarking:mode_settings:sources:default_match_field</code>.</p>
<p>Example: (see also <code>examples/benchmarking/queries.json</code>)</p>
<pre><code class="language-json">[
  {
    &quot;question&quot;: &quot;Does Jorian Benke work with the PPC and what topic will she work on?&quot;,
    &quot;sources&quot;: [&quot;https://ppc.mit.edu/blog/2025/07/14/welcome-our-first-ever-in-house-masters-student/&quot;, &quot;CMSPROD-42&quot;],
    &quot;answer&quot;: &quot;Yes, Jorian works with the PPC and her topic is the study of Lorentz invariance.&quot;,
    &quot;source_match_field&quot;: [&quot;url&quot;, &quot;ticket_id&quot;]
  },
  ...
]
</code></pre>
<p>N.B.: one could also provide the <code>url</code> for the JIRA ticket: it is just a choice that you must make, and detail in <code>source_match_field</code>. i.e., the following will evaluate equivalently as the above example:</p>
<pre><code class="language-json">[
  {
    &quot;question&quot;: &quot;Does Jorian Benke work with the PPC and what topic will she work on?&quot;,
    &quot;sources&quot;: [&quot;https://ppc.mit.edu/blog/2025/07/14/welcome-our-first-ever-in-house-masters-student/&quot;, &quot;https://its.cern.ch/jira/browse/CMSPROD-42&quot;],
    &quot;answer&quot;: &quot;Yes, Jorian works with the PPC and her topic is the study of Lorentz invariance.&quot;,
    &quot;source_match_field&quot;: [&quot;url&quot;, &quot;url&quot;]
  },
  ...
]
</code></pre>
<h3 id="configuration_10">Configuration</h3>
<p>You can evaluate one or more configurations by specifying the <code>evaluate</code> command with the <code>-cd</code> flag pointing to the directory containing your configuration file(s). You can also specify individual files with the <code>-c</code> flag. This can be useful if you're interested in comparing different hyperparameter settings.</p>
<p>We support two modes, which you can specify in the configuration file under <code>services:benchmarking:modes</code>. You can choose either or both of <code>RAGAS</code> and <code>SOURCES</code>.</p>
<p>The RAGAS mode will use the Ragas RAG evaluator module to return numerical values judging by 4 of their provided metrics: <code>answer_relevancy</code>, <code>faithfulness</code>, <code>context precision</code>, and <code>context relevancy</code>. More information about these metrics can be found on the <a href="https://docs.ragas.io/en/stable/concepts/metrics/">Ragas website</a>. </p>
<p>The SOURCES mode will check if the retrieved documents contain any of the correct sources. The matching is done by comparing a given metadata field for any source. The default is <code>display_name</code>, as per the configuration file (<code>data_manager:services:benchmarking:mode_settings:sources:default_match_field</code>). You can override this on a per-query basis by specifying the <code>sources_match_field</code> field in the queries file, as described above.</p>
<p>The configuration file should look like the following:</p>
<pre><code class="language-yaml">services:
  benchmarking:
    queries_path: examples/benchmarking/queries.json
    out_dir: bench_out
    modes:
      - &quot;RAGAS&quot;
      - &quot;SOURCES&quot;
    mode_settings:
      sources:
        default_match_field: [&quot;display_name&quot;] # default field to match sources against, can be overridden in the queries file
      ragas_settings:
        provider: &lt;provider name&gt; # can be one of OpenAI, HuggingFace, Ollama, and Anthropic
        evaluation_model_settings:
          model_name: &lt;model name&gt; # ensure this lines up with the langchain API name for your chosen model and provider
          base_url: &lt;url&gt; # address to your running Ollama server should you have chosen the Ollama provider
        embedding_model: &lt;embedding provider&gt; # OpenAI or HuggingFace
</code></pre>
<p>Finally, before you run the command ensure <code>out_dir</code>, the output directory, both exists on your system and that the path is correctly specified so that results can show up inside of it.</p>
<h3 id="running_9">Running</h3>
<p>To run the benchmarking script simply run the following:</p>
<pre><code class="language-bash">a2rchi evaluate -n &lt;name&gt; -e &lt;env_file&gt; -cd &lt;configs_directory&gt; &lt;optionally use  -c &lt;file1&gt;,&lt;file2&gt;, ...&gt; &lt;OPTIONS&gt;
</code></pre>
<p>Example:</p>
<pre><code class="language-bash">a2rchi evaluate -n benchmark -c examples/benchmarking/benchmark_configs/example_conf.yaml --gpu-ids all
</code></pre>
<h3 id="additional-options">Additional options</h3>
<p>You might also want to adjust the <code>timeout</code> setting, which is the upper limit on how long the Ragas evaluation takes on a single QA pair, or the <code>batch_size</code>, which determines how many QA pairs to evaluate at once, which you might want to adjust, e.g., based on hardware constraints, as Ragas doesn't pay great attention to that. The corresponding configuration options are similarly set for the benchmarking services, as follows:</p>
<pre><code class="language-yaml">services:
  benchmarking:
    timeout: &lt;time in seconds&gt; # default is 180
    batch_size: &lt;desired batch size&gt; # no default setting, set by Ragas...
</code></pre>
<h3 id="results">Results</h3>
<p>The output of the benchmarking will be saved in the <code>out_dir</code> specified in the configuration file. The results will be saved in a timestamped subdirectory, e.g., <code>bench_out/2042-10-01_12-00-00/</code>.</p>
<p>To later examine your data, check out <code>scripts/benchmarking/</code>, which contains some plotting functions and an ipynotebook with some basic usage examples. This is useful to play around with the results of the benchmarking, we will soon also have instead dedicated scripts to produce the plots of interest.</p>
<hr />
<h2 id="other">Other</h2>
<p>Some useful additional features supported by the framework.</p>
<h3 id="add-chromadb-document-management-api-endpoints">Add ChromaDB Document Management API Endpoints</h3>
<h5 id="debugging-chromadb-endpoints">Debugging ChromaDB endpoints</h5>
<p>Debugging REST API endpoints to the A2RCHI chat application for programmatic access to the ChromaDB vector database can be exposed with the following configuration change.
To enable the ChromaDB endpoints, add the following to your config file under <code>services.chat_app</code>:</p>
<pre><code class="language-yaml">services:
  chat_app:
    # ... other config options ...
    enable_debug_chroma_endpoints: true  # Default: false
</code></pre>
<h6 id="chromadb-endpoints-info">ChromaDB  Endpoints Info</h6>
<h6 id="apilist_docs-get"># <code>/api/list_docs</code> (GET)</h6>
<p>Lists all documents indexed in ChromaDB with pagination support.</p>
<p><strong>Query Parameters:</strong>
- <code>page</code>: Page number (1-based, default: 1)
- <code>per_page</code>: Documents per page (default: 50, max: 500)
- <code>content_length</code>: Content preview length (default: -1 for full content)</p>
<p><strong>Response:</strong></p>
<pre><code class="language-json">{
  &quot;pagination&quot;: {
    &quot;page&quot;: 1,
    &quot;per_page&quot;: 50,
    &quot;total_documents&quot;: 1250,
    &quot;total_pages&quot;: 25,
    &quot;has_next&quot;: true,
    &quot;has_prev&quot;: false
  },
  &quot;documents&quot;: [...]
}
</code></pre>
<h6 id="apisearch_docs-post"># <code>/api/search_docs</code> (POST)</h6>
<p>Performs semantic search on the document collection using vector similarity.</p>
<p><strong>Request Body:</strong>
- <code>query</code>: Search query string (required)
- <code>n_results</code>: Number of results (default: 5, max: 100)
- <code>content_length</code>: Max content length (default: -1, max: 5000)
- <code>include_full_content</code>: Include complete document content (default: false)</p>
<p><strong>Response:</strong></p>
<pre><code class="language-json">{
  &quot;query&quot;: &quot;machine learning&quot;,
  &quot;search_params&quot;: {...},
  &quot;documents&quot;: [
    {
      &quot;content&quot;: &quot;Document content...&quot;,
      &quot;content_length&quot;: 1200,
      &quot;metadata&quot;: {...},
      &quot;similarity_score&quot;: 0.85
    }
  ]
}
</code></pre>
<hr />
              
            </div>
          </div><footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="Footer Navigation">
        <a href="../quickstart/" class="btn btn-neutral float-left" title="Quickstart"><span class="icon icon-circle-arrow-left"></span> Previous</a>
        <a href="../advanced_setup_deploy/" class="btn btn-neutral float-right" title="Advanced Setup and Deployment">Next <span class="icon icon-circle-arrow-right"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
  </div>

  Built with <a href="https://www.mkdocs.org/">MkDocs</a> using a <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
          
        </div>
      </div>

    </section>

  </div>

  <div class="rst-versions" role="note" aria-label="Versions">
  <span class="rst-current-version" data-toggle="rst-current-version">
    
        <span>
          <a href="https://github.com/mit-submit/A2rchi" class="fa fa-github" style="color: #fcfcfc"> GitHub</a>
        </span>
    
    
      <span><a href="../quickstart/" style="color: #fcfcfc">&laquo; Previous</a></span>
    
    
      <span><a href="../advanced_setup_deploy/" style="color: #fcfcfc">Next &raquo;</a></span>
    
  </span>
</div>
    <script src="../js/jquery-3.6.0.min.js"></script>
    <script>var base_url = "..";</script>
    <script src="../js/theme_extra.js"></script>
    <script src="../js/theme.js"></script>
      <script src="../search/main.js"></script>
    <script>
        jQuery(function () {
            SphinxRtdTheme.Navigation.enable(true);
        });
    </script>

</body>
</html>
