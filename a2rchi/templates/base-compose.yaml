services:
  {% if not use_grader_service -%}
  chat:
    image: {{ chat_image }}:{{ chat_tag }}
    build:
      context: .
      dockerfile: a2rchi_code/templates/dockerfiles/Dockerfile-chat{{ '-gpu' if gpu_ids else '' }}
    container_name: {{ chat_container_name }}
    depends_on:
      chromadb:
        condition: service_healthy
      postgres:
        condition: service_healthy
    environment:
      {% if sso -%}
      SSO_USERNAME_FILE: /run/secrets/sso_username
      SSO_PASSWORD_FILE: /run/secrets/sso_password
      {% endif %}
      {% if openai -%}
      OPENAI_API_KEY_FILE: /run/secrets/openai_api_key
      {% endif %}
      {% if anthropic -%}
      ANTHROPIC_API_KEY_FILE: /run/secrets/anthropic_api_key
      {% endif %}
      {% if huggingface -%}
      HUGGING_FACE_HUB_TOKEN_FILE: /run/secrets/hf_token
      {% endif %}
      POSTGRES_PASSWORD_FILE: /run/secrets/pg_password
      {% if jira -%}
      JIRA_PAT_FILE: /run/secrets/jira_pat
      {% endif %}
      {% if gpu_ids -%}
      NVIDIA_VISIBLE_DEVICES: all
      NVIDIA_DRIVER_CAPABILITIES: compute,utility,graphics
      {% endif %}
    {% if gpu_ids and not use_podman -%}
    # TODO: docker GPU configuration needs testing
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    {% endif %}
    secrets:
      {% if sso -%}
      - sso_username
      - sso_password
      {% endif %}
      {% if openai -%}
      - openai_api_key
      {% endif %}
      {% if anthropic -%}
      - anthropic_api_key
      {% endif %}
      {% if huggingface -%}
      - hf_token
      {% endif %}
      - pg_password
      {% if jira -%}
      - jira_pat
      {% endif %}
    volumes:
      - {{ chat_volume_name }}:/root/data/
      - ./config.yaml:/root/A2rchi/config.yaml
      - ./main.prompt:/root/A2rchi/main.prompt
      - ./condense.prompt:/root/A2rchi/condense.prompt
      {% if gpu_ids -%}
      - a2rchi-models:/root/models/
      {%- endif %}
    logging:
      options:
        max-size: 10m
    ports:
      - {{ chat_port_host }}:{{ chat_port_container }}  # host:container
    restart: always
    {% if host_mode -%}
    network_mode: host
    {% endif %}
    {% if gpu_ids and use_podman -%}
    # podman GPU configuration
    security_opt:
      - label:disable
    devices:
    # N.B. even smallest models will load across multiple GPUs if available.
    {%- if gpu_ids == "all" %}
      - "nvidia.com/gpu=all"
    {%- else %}
      {%- for gpu_id in gpu_ids %}
      - "nvidia.com/gpu={{ gpu_id }}"
      {%- endfor %}
    {%- endif %}
    {%- endif %}
  {%- endif %}
  
  chromadb:
    image: {{ chromadb_image }}:{{ chromadb_tag }}
    build:
      context: .
      dockerfile:  a2rchi_code/templates/dockerfiles/Dockerfile-chroma
    container_name: {{ chromadb_container_name }}
    ports:
      - {{ chromadb_port_host }}:8000  # host:container
    volumes:
      - {{ chat_volume_name }}:/chroma/chroma/
    logging:
      options:
        max-size: 10m
    restart: always
    {% if host_mode -%}
    network_mode: host
    {% endif %}
    # healthcheck originates from inside container; so use container port
    healthcheck:
      test: ["CMD", "curl", "-f", "http://0.0.0.0:8000/api/v1/heartbeat"] #health check uses container port
      interval: 15s
      timeout: 10s
      retries: 3
      start_period: 10s
      start_interval: 5s

  postgres:
    image: docker.io/postgres:16
    container_name: {{ postgres_container_name }}
    environment:
      POSTGRES_PASSWORD_FILE: /run/secrets/pg_password
      POSTGRES_USER: a2rchi
      POSTGRES_DB: a2rchi-db
    secrets:
      - pg_password
    volumes:
      - ./init.sql:/docker-entrypoint-initdb.d/init.sql
      - {{ postgres_volume_name }}:/var/lib/postgresql/data
    logging:
      options:
        max-size: 10m
    restart: always
    {% if host_mode -%}
    network_mode: host
    {% endif %}
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U a2rchi -d a2rchi-db"]
      interval: 10s
      timeout: 5s
      retries: 5

  {% if use_grafana -%}
  grafana:
    image: {{ grafana_image }}:{{ grafana_tag }}
    build:
      context: .
      dockerfile:  a2rchi_code/templates/dockerfiles/Dockerfile-grafana
    container_name: {{ grafana_container_name }}
    depends_on:
      postgres:
        condition: service_healthy
    ports:
      - {{ grafana_port_host }}:3000  # host:container
    volumes:
      - {{ grafana_volume_name }}:/var/lib/grafana
      - ./grafana/a2rchi-default-dashboard.json:/var/lib/grafana/dashboards/a2rchi-default-dashboard.json
      - ./grafana/datasources.yaml:/etc/grafana/provisioning/datasources/datasources.yaml
      - ./grafana/dashboards.yaml:/etc/grafana/provisioning/dashboards/dashboards.yaml
      - ./grafana/grafana.ini:/etc/grafana/grafana.ini
    logging:
      options:
        max-size: 10m
    restart: always
    {% if host_mode -%}
    network_mode: host
    {% endif %}
  {%- endif %}

  {% if use_uploader_service -%}
  uploader:
    image: {{ uploader_image }}:{{ uploader_tag }}
    build:
      context: .
      dockerfile: a2rchi_code/templates/dockerfiles/Dockerfile-uploader
    depends_on:
      chromadb:
        condition: service_healthy
      postgres:
        condition: service_healthy
    environment:
      FLASK_UPLOADER_APP_SECRET_KEY_FILE: /run/secrets/flask_uploader_app_secret_key
      UPLOADER_SALT_FILE: /run/secrets/uploader_salt
      {% if openai -%}
      OPENAI_API_KEY_FILE: /run/secrets/openai_api_key
      {% endif %}
      {% if anthropic -%}
      ANTHROPIC_API_KEY_FILE: /run/secrets/anthropic_api_key
      {% endif %}
      {% if huggingface -%}
      HUGGING_FACE_HUB_TOKEN_FILE: /run/secrets/hf_token
      {% endif %}
    secrets:
      - flask_uploader_app_secret_key
      - uploader_salt
      {% if openai -%}
      - openai_api_key
      {% endif %}
      {% if anthropic -%}
      - anthropic_api_key
      {% endif %}
      {% if huggingface -%}
      - hf_token
      {% endif %}
    ports:
      - {{ uploader_port_host }}:{{ uploader_port_container }}  # host:container
    volumes:
      - {{ chat_volume_name }}:/root/data/
      - ./config.yaml:/root/A2rchi/config.yaml
    logging:
      options:
        max-size: 10m
    restart: always
    {% if host_mode -%}
    network_mode: host
    {% endif %}
  {%- endif %}

  {% if use_grader_service -%}
  grader:
    image: {{ grader_image }}:{{ grader_tag }}
    build:
      context: .
      dockerfile: a2rchi_code/templates/dockerfiles/Dockerfile-grader{{ '-gpu' if gpu_ids else '' }}
    depends_on:
      chromadb:
        condition: service_healthy
      postgres:
        condition: service_healthy
    environment:
      {% if openai -%}
      OPENAI_API_KEY_FILE: /run/secrets/openai_api_key
      {% endif %}
      {% if anthropic -%}
      ANTHROPIC_API_KEY_FILE: /run/secrets/anthropic_api_key
      {% endif %}
      {% if huggingface -%}
      HUGGING_FACE_HUB_TOKEN_FILE: /run/secrets/hf_token
      {% endif %}
      POSTGRES_PASSWORD_FILE: /run/secrets/pg_password
      {% if gpu_ids -%}
      NVIDIA_VISIBLE_DEVICES: all
      NVIDIA_DRIVER_CAPABILITIES: compute,utility,graphics
      {%- endif %}
      USERS_FILE: /run/secrets/users
      ADMIN_PASSWORD_FILE: /run/secrets/admin_password
      {% for rubric in rubrics | default([]) %}
      {{ rubric }}_FILE: /run/secrets/{{ rubric }}
      {%- endfor %}
    secrets:
      {% if openai -%}
      - openai_api_key
      {% endif %}
      {% if anthropic -%}
      - anthropic_api_key
      {% endif %}
      {% if huggingface -%}
      - hf_token
      {% endif %}
      - pg_password
      - admin_password
    ports: 
      - {{ grader_port_host }}:{{ grader_port_container }}  # host:container
    volumes:
      - {{ grader_volume_name }}:/root/data/
      - ./config.yaml:/root/A2rchi/config.yaml
      - ./image_processing.prompt:/root/A2rchi/image_processing.prompt
      - ./grading_final_grade.prompt:/root/A2rchi/grading_final_grade.prompt
      {% if analysis -%}
      - ./grading_analysis.prompt:/root/A2rchi/grading_analysis.prompt
      {%- endif %}
      {% if summary -%}
      - ./grading_summary.prompt:/root/A2rchi/grading_summary.prompt
      {%- endif %}
      - ./users.csv:/root/A2rchi/users.csv
      {%- for rubric in rubrics | default([]) %}
      - ./{{ rubric }}.txt:/root/A2rchi/{{ rubric }}.txt
      {%- endfor %}
      {% if gpu_ids -%}
      - a2rchi-models:/root/models/
      {%- endif %}
      # TODO: conditionally include grading summary and analysis prompts
    logging:
      options:
        max-size: 10m
    restart: always
    {% if host_mode -%}
    network_mode: host
    {% endif %}
    {% if gpu_ids and use_podman -%}
    # podman GPU configuration
    security_opt:
      - label:disable
    devices:
    # N.B. even smallest models will load across multiple GPUs if available.
    {%- if gpu_ids == "all" %}
      - "nvidia.com/gpu=all"
    {%- else %}
      {%- for gpu_id in gpu_ids %}
      - "nvidia.com/gpu={{ gpu_id }}"
      {%- endfor %}
    {%- endif %}
    {%- endif %}
  {%- endif %}

  {% if use_piazza_service -%}
  piazza:
    image: {{ piazza_image }}:{{ piazza_tag }}
    build:
      context: .
      dockerfile: a2rchi_code/templates/dockerfiles/Dockerfile-piazza
    depends_on:
      chromadb:
        condition: service_healthy
      postgres:
        condition: service_healthy
    environment:
      {% if openai -%}
      OPENAI_API_KEY_FILE: /run/secrets/openai_api_key
      {% endif %}
      {% if anthropic -%}
      ANTHROPIC_API_KEY_FILE: /run/secrets/anthropic_api_key
      {% endif %}
      {% if huggingface -%}
      HUGGING_FACE_HUB_TOKEN_FILE: /run/secrets/hf_token
      {% endif %}
      PIAZZA_EMAIL_FILE: /run/secrets/piazza_email
      PIAZZA_PASSWORD_FILE: /run/secrets/piazza_password
      SLACK_WEBHOOK_FILE: /run/secrets/slack_webhook
    secrets:
      {% if openai -%}
      - openai_api_key
      {% endif %}
      {% if anthropic -%}
      - anthropic_api_key
      {% endif %}
      {% if huggingface -%}
      - hf_token
      {% endif %}
      - piazza_email
      - piazza_password
      - slack_webhook
    volumes:
      - {{ chat_volume_name }}:/root/data/
      - ./config.yaml:/root/A2rchi/config.yaml
      - ./main.prompt:/root/A2rchi/main.prompt
      - ./condense.prompt:/root/A2rchi/condense.prompt
      - ./summary.prompt:/root/A2rchi/summary.prompt
      {% if gpu_ids -%}
      - a2rchi-models:/root/models/
      {%- endif %}
    logging:
      options:
        max-size: 10m
    restart: always
    {% if host_mode -%}
    network_mode: host
    {% endif %}
  {%- endif %}

  {% if use_mattermost_service -%}
  mattermost:
    image: {{ mattermost_image }}:{{ mattermost_tag }}
    build:
      context: .
      dockerfile: a2rchi_code/templates/dockerfiles/Dockerfile-mattermost{{ '-gpu' if gpu_ids else '' }}      
      args:
        TAG: {{ mattermost_tag }}
    depends_on:
      chromadb:
        condition: service_healthy
      postgres:
        condition: service_healthy
    environment:
      {% if openai -%}
      OPENAI_API_KEY_FILE: /run/secrets/openai_api_key
      {% endif %}
      {% if anthropic -%}
      ANTHROPIC_API_KEY_FILE: /run/secrets/anthropic_api_key
      {% endif %}
      {% if huggingface -%}
      HUGGING_FACE_HUB_TOKEN_FILE: /run/secrets/hf_token
      {% endif %}
      {% if gpu_ids -%}
      NVIDIA_VISIBLE_DEVICES: all
      NVIDIA_DRIVER_CAPABILITIES: compute,utility,graphics
      {% endif %}
      MATTERMOST_WEBHOOK_FILE: /run/secrets/mattermost_webhook
      MATTERMOST_CHANNEL_ID_READ_FILE: /run/secrets/mattermost_channel_id_read
      MATTERMOST_CHANNEL_ID_WRITE_FILE: /run/secrets/mattermost_channel_id_write
      MATTERMOST_PAK_FILE: /run/secrets/mattermost_pak
    {% if gpu_ids and not use_podman -%}
    # TODO: docker GPU configuration needs testing
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    {% endif %}
    secrets:
      {% if openai -%}
      - openai_api_key
      {% endif %}
      {% if anthropic -%}
      - anthropic_api_key
      {% endif %}
      {% if huggingface -%}
      - hf_token
      {% endif %}
      - mattermost_webhook
      - mattermost_channel_id_read
      - mattermost_channel_id_write
      - mattermost_pak
    volumes:
      - {{ chat_volume_name }}:/root/data/
      - ./config.yaml:/root/A2rchi/config.yaml
      - ./main.prompt:/root/A2rchi/main.prompt
      - ./condense.prompt:/root/A2rchi/condense.prompt
      - ./summary.prompt:/root/A2rchi/summary.prompt
      {% if gpu_ids -%}
      - a2rchi-models:/root/models/
      {%- endif %}
    logging:
      options:
        max-size: 10m
    restart: always
    {% if host_mode -%}
    network_mode: host
    {% endif %}
    {% if gpu_ids and use_podman -%}
    # podman GPU configuration
    security_opt:
      - label:disable
    devices:
    # N.B. even smallest models will load across multiple GPUs if available.
    {%- if gpu_ids == "all" %}
      - "nvidia.com/gpu=all"
    {%- else %}
      {%- for gpu_id in gpu_ids %}
      - "nvidia.com/gpu={{ gpu_id }}"
      {%- endfor %}
    {%- endif %}
    {%- endif %}    
  {% endif %}

  {% if use_cleo_and_mailer -%}
  cleo:
    image: {{ cleo_image }}:{{ cleo_tag }}
    build:
      context: .
      dockerfile: a2rchi_code/templates/dockerfiles/Dockerfile-cleo{{ '-gpu' if gpu_ids else '' }}
    depends_on:
      chromadb:
        condition: service_healthy
      postgres:
        condition: service_healthy
    environment:
      CLEO_URL_FILE: /run/secrets/cleo_url
      CLEO_USER_FILE: /run/secrets/cleo_user
      CLEO_PW_FILE: /run/secrets/cleo_pw
      CLEO_PROJECT_FILE: /run/secrets/cleo_project
      SENDER_SERVER_FILE: /run/secrets/sender_server
      SENDER_PORT_FILE: /run/secrets/sender_port
      SENDER_REPLYTO_FILE: /run/secrets/sender_replyto
      SENDER_USER_FILE: /run/secrets/sender_user
      SENDER_PW_FILE: /run/secrets/sender_pw
      {% if openai -%}
      OPENAI_API_KEY_FILE: /run/secrets/openai_api_key
      {% endif %}
      {% if anthropic -%}
      ANTHROPIC_API_KEY_FILE: /run/secrets/anthropic_api_key
      {% endif %}
      {% if huggingface -%}
      HUGGING_FACE_HUB_TOKEN_FILE: /run/secrets/hf_token
      {% endif %}
      POSTGRES_PASSWORD_FILE: /run/secrets/pg_password
      {% if gpu_ids -%}
      NVIDIA_VISIBLE_DEVICES: all
      NVIDIA_DRIVER_CAPABILITIES: compute,utility,graphics
      {%- endif %}
    secrets:
      - cleo_url
      - cleo_user
      - cleo_pw
      - cleo_project
      - sender_server
      - sender_port
      - sender_replyto
      - sender_user
      - sender_pw
      {% if openai -%}
      - openai_api_key
      {% endif %}
      {% if anthropic -%}
      - anthropic_api_key
      {% endif %}
      {% if huggingface -%}
      - hf_token
      {% endif %}
      - pg_password
    volumes:
      - {{ chat_volume_name }}:/root/data/
      - ./config.yaml:/root/A2rchi/config.yaml
      - ./main.prompt:/root/A2rchi/main.prompt
      - ./condense.prompt:/root/A2rchi/condense.prompt
      {% if gpu_ids -%}
      - a2rchi-models:/root/models/
      {%- endif %}
    logging:
      options:
        max-size: 10m
    restart: always
    {% if host_mode -%}
    network_mode: host
    {% endif %}
    {% if gpu_ids and use_podman -%}
    # podman GPU configuration
    security_opt:
      - label:disable
    devices:
    # N.B. even smallest models will load across multiple GPUs if available.
    {%- if gpu_ids == "all" %}
      - "nvidia.com/gpu=all"
    {%- else %}
      {%- for gpu_id in gpu_ids %}
      - "nvidia.com/gpu={{ gpu_id }}"
      {%- endfor %}
    {%- endif %}
    {%- endif %}

  mailbox:
    image: {{ mailbox_image }}:{{ mailbox_tag }}
    build:
      context: .
      dockerfile: a2rchi_code/templates/dockerfiles/Dockerfile-mailbox{{ '-gpu' if gpu_ids else '' }}
    depends_on:
      chromadb:
        condition: service_healthy
      postgres:
        condition: service_healthy
    environment:
      IMAP_USER_FILE: /run/secrets/imap_user
      IMAP_PW_FILE: /run/secrets/imap_pw
      CLEO_URL_FILE: /run/secrets/cleo_url
      CLEO_USER_FILE: /run/secrets/cleo_user
      CLEO_PW_FILE: /run/secrets/cleo_pw
      CLEO_PROJECT_FILE: /run/secrets/cleo_project
      SENDER_SERVER_FILE: /run/secrets/sender_server
      SENDER_PORT_FILE: /run/secrets/sender_port
      SENDER_REPLYTO_FILE: /run/secrets/sender_replyto
      SENDER_USER_FILE: /run/secrets/sender_user
      SENDER_PW_FILE: /run/secrets/sender_pw
      {% if openai -%}
      OPENAI_API_KEY_FILE: /run/secrets/openai_api_key
      {% endif %}
      {% if anthropic -%}
      ANTHROPIC_API_KEY_FILE: /run/secrets/anthropic_api_key
      {% endif %}
      {% if huggingface -%}
      HUGGING_FACE_HUB_TOKEN_FILE: /run/secrets/hf_token
      {% endif %}
      {% if gpu_ids -%}
      NVIDIA_VISIBLE_DEVICES: all
      NVIDIA_DRIVER_CAPABILITIES: compute,utility,graphics
      {%- endif %}
    secrets:
      - imap_user
      - imap_pw
      - cleo_url
      - cleo_user
      - cleo_pw
      - cleo_project
      - sender_server
      - sender_port
      - sender_replyto
      - sender_user
      - sender_pw
      {% if openai -%}
      - openai_api_key
      {% endif %}
      {% if anthropic -%}
      - anthropic_api_key
      {% endif %}
      {% if huggingface -%}
      - hf_token
      {% endif %}
    volumes:
      - ./config.yaml:/root/A2rchi/config.yaml
      - ./main.prompt:/root/A2rchi/main.prompt
      - ./condense.prompt:/root/A2rchi/condense.prompt
      {% if gpu_ids -%}
      - a2rchi-models:/root/models/
      {%- endif %}
    logging:
      options:
        max-size: 10m
    restart: always
    {% if host_mode -%}
    network_mode: host
    {% endif %}
    {% if gpu_ids and use_podman -%}
    # podman GPU configuration
    security_opt:
      - label:disable
    devices:
    # N.B. even smallest models will load across multiple GPUs if available.
    {%- if gpu_ids == "all" %}
      - "nvidia.com/gpu=all"
    {%- else %}
      {%- for gpu_id in gpu_ids %}
      - "nvidia.com/gpu={{ gpu_id }}"
      {%- endfor %}
    {%- endif %}
    {%- endif %}
  {%- endif %}

volumes:
  {{ chat_volume_name }}:
    external: true
  {{ postgres_volume_name }}:
    external: true
  {% if use_grafana -%}
  {{ grafana_volume_name }}:
    external: true
  {%- endif %}
  {% if use_grader_service -%}
  {{ grader_volume_name }}:
    external: true
  {%- endif %}
  {% if gpu_ids -%}
  a2rchi-models:
    external: true
  {%- endif %}

secrets:
  # Core secrets
  openai_api_key:
    file: secrets/openai_api_key.txt
  anthropic_api_key:
    file: secrets/anthropic_api_key.txt
  hf_token:
    file: secrets/hf_token.txt
  pg_password:
    file: secrets/pg_password.txt

  # Uploader secrets
  flask_uploader_app_secret_key:
    file: secrets/flask_uploader_app_secret_key.txt
  uploader_salt:
    file: secrets/uploader_salt.txt

  # Cleo and mailbox secrets
  imap_user:
    file: secrets/imap_user.txt
  imap_pw:
    file: secrets/imap_pw.txt
  cleo_url:
    file: secrets/cleo_url.txt
  cleo_user:
    file: secrets/cleo_user.txt
  cleo_pw:
    file: secrets/cleo_pw.txt
  cleo_project:
    file: secrets/cleo_project.txt
  sender_server:
    file: secrets/sender_server.txt
  sender_port:
    file: secrets/sender_port.txt
  sender_replyto:
    file: secrets/sender_replyto.txt
  sender_user:
    file: secrets/sender_user.txt
  sender_pw:
    file: secrets/sender_pw.txt
  # Piazza secrets
  piazza_email:
    file: secrets/piazza_email.txt
  piazza_password:
    file: secrets/piazza_password.txt
  slack_webhook:
    file: secrets/slack_webhook.txt


  # Grader secrets
  admin_password:
    file: secrets/admin_password.txt

  # Mattermost secrets
  mattermost_webhook:
    file: secrets/mattermost_webhook.txt
  mattermost_channel_id_read:
    file: secrets/mattermost_channel_id_read.txt
  mattermost_channel_id_write:
    file: secrets/mattermost_channel_id_write.txt
  mattermost_pak:
    file: secrets/mattermost_pak.txt

  # JIRA secrets
  jira_pat:
    file: secrets/jira_pat.txt

  # SSO secrets
  sso_username:
    file: secrets/sso_username.txt
  sso_password:
    file: secrets/sso_password.txt