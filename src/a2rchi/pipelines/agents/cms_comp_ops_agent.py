from __future__ import annotations

from pathlib import Path
from typing import Any, Callable, Dict, List, Optional, Sequence

from langchain_core.messages import AIMessage, BaseMessage, HumanMessage, SystemMessage
from langgraph.graph.state import CompiledStateGraph
from langchain.agents import create_agent

from src.utils.logging import get_logger
from src.a2rchi.pipelines.agents.base import BaseAgent
from src.data_manager.vectorstore.retrievers.utils import SemanticRetriever
from src.a2rchi.tools.base import (
    CatalogService,
    create_file_search_tool,
    create_metadata_search_tool,
    create_retriever_tool,
)

logger = get_logger(__name__)


class CMSCompOpsAgent(BaseAgent):
    """Agent designed for CMS CompOps operations."""

    def __init__(
        self,
        config: Dict[str, Any],
        *args,
        **kwargs,
    ) -> None:
        super().__init__(config, *args, **kwargs)
        self.agent_llm = self.llms.get("chat_model") or next(iter(self.llms.values()))
        self.agent_prompt = self.prompts.get("agent_prompt")

        data_path = self.config["global"]["DATA_PATH"]
        self.catalog_service = CatalogService(data_path=data_path)

        self._vector_retriever = None
        self._vector_tool = None
        self._active_tools: Optional[List[Callable]] = None

        self.tools = self._build_static_tools()
        self.agent = self._create_agent(self.tools)

    def _create_agent(self, tools: Sequence[Callable]) -> CompiledStateGraph:
        """Create the LangGraph agent with the specified LLM, tools, and system prompt."""
        logger.debug("Creating CMSCompOpsAgent with %d tools", len(tools))
        return create_agent(
            model=self.agent_llm,
            tools=tools,
            system_prompt=self.agent_prompt,
        )

    def _build_static_tools(self) -> List[Callable]:
        """Initialise static tools that are always available to the agent."""
        configured_tools = list(self.pipeline_config.get("tools", []))
        file_search_tool = create_file_search_tool(
            self.catalog_service,
            description="Search local source files collected in the catalog.",
        )
        metadata_search_tool = create_metadata_search_tool(
            self.catalog_service,
            description="Search metadata associated with local source files.",
        )
        return configured_tools + [file_search_tool, metadata_search_tool]

    def _infer_speaker(self, speaker: str) -> type[BaseMessage]:
        """Infer the speaker type and return the appropriate message class."""
        if speaker.lower() in ["user", "human"]:
            return HumanMessage
        if speaker.lower() in ["agent", "ai", "assistant", "a2rchi"]:
            return AIMessage
        logger.warning("Unknown speaker type: %s. Defaulting to HumanMessage.", speaker)
        return HumanMessage

    def _prepare_inputs(self, history: Any, **kwargs) -> Dict[str, Any]:
        """Create list of messages using LangChain's formatting."""
        history = history or []
        history_messages = [self._infer_speaker(msg[0])(msg[1]) for msg in history]
        return {"history": history_messages}

    def _prepare_agent_inputs(self, **kwargs) -> Dict[str, Any]:
        """Prepare agent state and formatted inputs shared by invoke/stream."""
        vectorstore = kwargs.get("vectorstore")
        if vectorstore:
            self._update_vector_retriever(vectorstore)
        else:
            self._vector_retriever = None
            self._vector_tool = None

        # ensure the latest files are indexed for the tools' use
        self.catalog_service.refresh()

        toolset = list(self.tools)
        if self._vector_tool:
            toolset.append(self._vector_tool)

        if (
            self._active_tools is None
            or len(toolset) != len(self._active_tools)
            or any(a is not b for a, b in zip(toolset, self._active_tools))
        ):
            logger.debug("Refreshing agent graph with %d tools", len(toolset))
            self.agent = self._create_agent(toolset)
            self._active_tools = list(toolset)

        inputs = self._prepare_inputs(history=kwargs.get("history"))
        return {"messages": inputs["history"]}

    def invoke(self, **kwargs) -> Dict[str, Any]:
        logger.debug("Invoking CMSCompOpsAgent")

        agent_inputs = self._prepare_agent_inputs(**kwargs)

        answer_output = self.agent.invoke(agent_inputs)
        logger.debug("Agent invocation completed")
        logger.debug("Agent output: %s", answer_output)

        messages = answer_output.get("messages", [])
        if messages:
            answer_text = messages[-1].content
        else:
            answer_text = "No answer generated by the agent."

        return {"answer": answer_text, "documents": []}

    def stream(self, **kwargs):
        """Stream agent updates synchronously."""
        logger.debug("Streaming CMSCompOpsAgent")
        agent_inputs = self._prepare_agent_inputs(**kwargs)
        return self.agent.stream(agent_inputs, stream_mode="updates")

    async def astream(self, **kwargs):
        """Stream agent updates asynchronously."""
        logger.debug("Streaming CMSCompOpsAgent asynchronously")
        agent_inputs = self._prepare_agent_inputs(**kwargs)
        async for event in self.agent.astream(agent_inputs, stream_mode="updates"):
            yield event

    def _update_vector_retriever(self, vectorstore: Any) -> None:
        """Instantiate or refresh the vectorstore retriever tool."""
        search_kwargs = {"k": self.dm_config.get("num_documents_to_retrieve", 3)}

        if self.dm_config.get("use_hybrid_search", False):
            from A2rchi.A2rchi.src.data_manager.vectorstore.retrievers.utils import HybridRetriever

            retriever = HybridRetriever(
                vectorstore=vectorstore,
                search_kwargs=search_kwargs,
                bm25_weight=self.dm_config.get("bm25_weight", 0.6),
                semantic_weight=self.dm_config.get("semantic_weight", 0.4),
                bm25_k1=self.dm_config.get("bm25", {}).get("k1", 0.5),
                bm25_b=self.dm_config.get("bm25", {}).get("b", 0.75),
            )
        else:
            retriever = SemanticRetriever(
                vectorstore=vectorstore,
                search_kwargs=search_kwargs,
                dm_config=self.dm_config,
            )

        self._vector_retriever = retriever
        self._vector_tool = create_retriever_tool(
            retriever,
            name="search_vectorstore",
            description="Query the vectorstore built from local documents.",
        )
