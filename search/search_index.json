{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"A2rchi A2rchi (AI Augmented Research Chat Intelligence) is a retrieval-augmented generation (RAG) framework designed to be a low-barrier, open source, private, and customizable AI solution for research and educational support. A2rchi makes it easy to deploy AI assistants with a suite of tools to connect to various platforms (e.g., Piazza, Slack, Discourse, Mattermost, email) and knowledge bases (e.g., web links, files, JIRA tickets, documentation). It is designed to be modular and extensible, allowing users to easily add new connectors and customize the behavior of the AI assistants. A2rchi also allows the design of custom pipelines to process user queries, including tools for retrieval, generation, and post-processing. This makes it easy to create AI assistants that can handle a wide range of tasks, from answering simple questions to providing detailed explanations and support. About A2rchi is an open source, end-to-end framework to quickly and easily build an AI support agent for classes and research resources developed jointly by Prof. Paus, MIT Physics and Prof. Kraska, MIT EECS, and their students. It has already been successfully deployed as an user chatbot and technical assistant at SubMIT, the private cluster at MIT\u2019s Physics Department, and as an educational assistant for several courses at MIT, including the 8.01 and 8.511, among others. Read below some example deployments and use cases. A2rchi is not the first or only AI support agent framework, but what we believe sets it apart is that it is fully open-source and customizable, works with different foundational models and LLM libraries, makes it easy to design custom AI-pipelines for your needs, and comes with a suite of services to interact with various popular platforms like Piazza, Redmine, Slack, JIRA, Discourse, and Mattermost, among others. As such, it can be entirely locally deployed and restricted to locally hosted open-source foundational models; a requirement whenever potential sensitive (student, user) data is involved. Under the hood, A2rchi is a highly configurable Retrieval Augmented Generation (RAG) system specifically designed for education and support for science resources. Given the success of A2rchi so far we are now expanding its scope to more MIT classes, CERN, Harvard, and other internal use cases (e.g., support for CSAIL\u2019s support staff). Educational Support A2rchi can be used as a tool for TAs, lecturers, and support staff to prepare answers or directly by students to receive help without human involvement. For example, in the A2rchi deployments to MIT courses, A2rchi provides high-quality answers by using previous Piazza posts, student questions, documentation, and other class/system-specific documents. Using the Piazza plug-in, A2rchi can help to prepare an answer based on a student's question. The TA/lecturer can then approve or modify the answer before sending it out, or A2rchi can send out the reply directly. Moreover, A2rchi continuously learns from any potential modification or from other Piazza posts, which are answers by TAs. So the answers keep improving over time, and are highly class-specific. Research Resource Support A2rchi can be used by technical support staff to prepare answers or directly by users to receive help without human involvement. For example, in the deployment to SubMIT, the MIT Physics Department's computing cluster, A2rchi is used both as an user-facing chatbot to answer questions and provide expert assistance, and as an assistant to the technical support staff by integrating with the Redmine ticketing system, where A2rchi prepares answers to the user's tickets, which can then be reviewed by the support staff before being answered. In both cases, A2rchi has access to the corpus of all tickets and documentation from the SubMIT cluster to learn from, and can provide specific sources as part of its answer.","title":"A2rchi"},{"location":"#a2rchi","text":"A2rchi (AI Augmented Research Chat Intelligence) is a retrieval-augmented generation (RAG) framework designed to be a low-barrier, open source, private, and customizable AI solution for research and educational support. A2rchi makes it easy to deploy AI assistants with a suite of tools to connect to various platforms (e.g., Piazza, Slack, Discourse, Mattermost, email) and knowledge bases (e.g., web links, files, JIRA tickets, documentation). It is designed to be modular and extensible, allowing users to easily add new connectors and customize the behavior of the AI assistants. A2rchi also allows the design of custom pipelines to process user queries, including tools for retrieval, generation, and post-processing. This makes it easy to create AI assistants that can handle a wide range of tasks, from answering simple questions to providing detailed explanations and support.","title":"A2rchi"},{"location":"#about","text":"A2rchi is an open source, end-to-end framework to quickly and easily build an AI support agent for classes and research resources developed jointly by Prof. Paus, MIT Physics and Prof. Kraska, MIT EECS, and their students. It has already been successfully deployed as an user chatbot and technical assistant at SubMIT, the private cluster at MIT\u2019s Physics Department, and as an educational assistant for several courses at MIT, including the 8.01 and 8.511, among others. Read below some example deployments and use cases. A2rchi is not the first or only AI support agent framework, but what we believe sets it apart is that it is fully open-source and customizable, works with different foundational models and LLM libraries, makes it easy to design custom AI-pipelines for your needs, and comes with a suite of services to interact with various popular platforms like Piazza, Redmine, Slack, JIRA, Discourse, and Mattermost, among others. As such, it can be entirely locally deployed and restricted to locally hosted open-source foundational models; a requirement whenever potential sensitive (student, user) data is involved. Under the hood, A2rchi is a highly configurable Retrieval Augmented Generation (RAG) system specifically designed for education and support for science resources. Given the success of A2rchi so far we are now expanding its scope to more MIT classes, CERN, Harvard, and other internal use cases (e.g., support for CSAIL\u2019s support staff).","title":"About"},{"location":"#educational-support","text":"A2rchi can be used as a tool for TAs, lecturers, and support staff to prepare answers or directly by students to receive help without human involvement. For example, in the A2rchi deployments to MIT courses, A2rchi provides high-quality answers by using previous Piazza posts, student questions, documentation, and other class/system-specific documents. Using the Piazza plug-in, A2rchi can help to prepare an answer based on a student's question. The TA/lecturer can then approve or modify the answer before sending it out, or A2rchi can send out the reply directly. Moreover, A2rchi continuously learns from any potential modification or from other Piazza posts, which are answers by TAs. So the answers keep improving over time, and are highly class-specific.","title":"Educational Support"},{"location":"#research-resource-support","text":"A2rchi can be used by technical support staff to prepare answers or directly by users to receive help without human involvement. For example, in the deployment to SubMIT, the MIT Physics Department's computing cluster, A2rchi is used both as an user-facing chatbot to answer questions and provide expert assistance, and as an assistant to the technical support staff by integrating with the Redmine ticketing system, where A2rchi prepares answers to the user's tickets, which can then be reviewed by the support staff before being answered. In both cases, A2rchi has access to the corpus of all tickets and documentation from the SubMIT cluster to learn from, and can provide specific sources as part of its answer.","title":"Research Resource Support"},{"location":"advanced_setup_deploy/","text":"Advanced Setup & Deployment Topics related to advanced setup and deployment of A2rchi. Configuring Podman To ensure your Podman containers stay running for extended periods, you need to enable lingering. To do this, the following command should work: loginctl enable-linger To check/confirm the lingering status, simply do loginctl user-status | grep -m1 Linger See the redhat docs to read more. Running LLMs locally on your GPUs There are a few additional system requirements for this to work: First, make sure you have nvidia drivers installed. (Optional) For the containers where A2rchi will run to access the GPUs, please install the nvidia container toolkit . If you are using one of the supported services, these containers are already configured to use GPUs. Configure the container runtime to access the GPUs. For Podman For Podman, run bash sudo nvidia-ctk cdi generate --output=/etc/cdi/nvidia.yaml Then, the following command nvidia-ctk cdi list should show an output that includes nohighlight INFO[0000] Found 9 CDI devices ... nvidia.com/gpu=0 nvidia.com/gpu=1 ... nvidia.com/gpu=all ... These listed \"CDI devices\" will be referenced to run A2rchi on the GPUs, so make sure this is there. To see more about accessing GPUs with Podman, click here . For Docker If you have Docker, run bash sudo nvidia-ctk runtime configure --runtime=docker What follows should be the same as above -- NOTE: this has not been tested yet with Docker. To see more about accessing GPUs with Docker, click here . Once these requirements are met, the a2rchi create [...] --gpu-ids <gpus> option will deploy A2rchi across your GPUs. Helpful Notes for Production Deployments You may wish to use the CLI in order to stage production deployments. This section covers some useful notes to keep in mind. Running multiple deployments on the same machine The CLI is built to allow multiple deployments to run on the same daemon in the case of Docker (Podman has no daemon). The container networks between all the deployments are separate, so there is very little risk of them accidentally communicating with one another. However, you need to be careful with the external ports. Suppose you're running two deployments and both of them are running the chat on external port 8000. There is no way to view both deployments at the same time from the same port, so instead you should split to forwarding the deployments to other external ports. Generally, this can be done in the configuration: interfaces: chat_app: EXTERNAL_PORT: 7862 # default is 7681 uploader_app: EXTERNAL_PORT: 5004 # default is 5003 grafana: EXTERNAL_PORT: 3001 # default is 3000 utils: data_manager: chromadb_external_port: 8001 # default is 8000 Persisting data between deployments Volumes persist between deployments, so if you deploy an instance, and upload some further documents, you will not need to redo this every time you deploy. Of course, if you are editing any data, you should explicitly remove this information from the volume, or simply remove the volume itself with docker/podman volume rm <volume-name> You can see what volumes are currently up with docker/podman volume ls","title":"Advanced Setup and Deployment"},{"location":"advanced_setup_deploy/#advanced-setup-deployment","text":"Topics related to advanced setup and deployment of A2rchi.","title":"Advanced Setup &amp; Deployment"},{"location":"advanced_setup_deploy/#configuring-podman","text":"To ensure your Podman containers stay running for extended periods, you need to enable lingering. To do this, the following command should work: loginctl enable-linger To check/confirm the lingering status, simply do loginctl user-status | grep -m1 Linger See the redhat docs to read more.","title":"Configuring Podman"},{"location":"advanced_setup_deploy/#running-llms-locally-on-your-gpus","text":"There are a few additional system requirements for this to work: First, make sure you have nvidia drivers installed. (Optional) For the containers where A2rchi will run to access the GPUs, please install the nvidia container toolkit . If you are using one of the supported services, these containers are already configured to use GPUs. Configure the container runtime to access the GPUs. For Podman For Podman, run bash sudo nvidia-ctk cdi generate --output=/etc/cdi/nvidia.yaml Then, the following command nvidia-ctk cdi list should show an output that includes nohighlight INFO[0000] Found 9 CDI devices ... nvidia.com/gpu=0 nvidia.com/gpu=1 ... nvidia.com/gpu=all ... These listed \"CDI devices\" will be referenced to run A2rchi on the GPUs, so make sure this is there. To see more about accessing GPUs with Podman, click here . For Docker If you have Docker, run bash sudo nvidia-ctk runtime configure --runtime=docker What follows should be the same as above -- NOTE: this has not been tested yet with Docker. To see more about accessing GPUs with Docker, click here . Once these requirements are met, the a2rchi create [...] --gpu-ids <gpus> option will deploy A2rchi across your GPUs.","title":"Running LLMs locally on your GPUs"},{"location":"advanced_setup_deploy/#helpful-notes-for-production-deployments","text":"You may wish to use the CLI in order to stage production deployments. This section covers some useful notes to keep in mind.","title":"Helpful Notes for Production Deployments"},{"location":"advanced_setup_deploy/#running-multiple-deployments-on-the-same-machine","text":"The CLI is built to allow multiple deployments to run on the same daemon in the case of Docker (Podman has no daemon). The container networks between all the deployments are separate, so there is very little risk of them accidentally communicating with one another. However, you need to be careful with the external ports. Suppose you're running two deployments and both of them are running the chat on external port 8000. There is no way to view both deployments at the same time from the same port, so instead you should split to forwarding the deployments to other external ports. Generally, this can be done in the configuration: interfaces: chat_app: EXTERNAL_PORT: 7862 # default is 7681 uploader_app: EXTERNAL_PORT: 5004 # default is 5003 grafana: EXTERNAL_PORT: 3001 # default is 3000 utils: data_manager: chromadb_external_port: 8001 # default is 8000","title":"Running multiple deployments on the same machine"},{"location":"advanced_setup_deploy/#persisting-data-between-deployments","text":"Volumes persist between deployments, so if you deploy an instance, and upload some further documents, you will not need to redo this every time you deploy. Of course, if you are editing any data, you should explicitly remove this information from the volume, or simply remove the volume itself with docker/podman volume rm <volume-name> You can see what volumes are currently up with docker/podman volume ls","title":"Persisting data between deployments"},{"location":"api_reference/","text":"API Reference CLI The A2rchi CLI provides commands to create, manage, and delete A2rchi deployments and services. Commands 1. create Create a new A2rchi deployment. Usage: a2rchi create --name <deployment_name> --config <config.yaml> --env-file <secrets.env> [OPTIONS] Options: --name, -n (str, required): Name of the deployment. --config, -c (str, required): Path to the YAML configuration file. --env-file, -e (str, required): Path to the secrets .env file. --services, -s (comma-separated): List of services to enable (e.g., chat_app,uploader_app ). --sources, -src (comma-separated): Data sources to enable (e.g., jira,redmine ). --podman, -p : Use Podman instead of Docker. --gpu-ids : GPU configuration ( all or comma-separated IDs). --tag, -t (str): Image tag for built containers (default: 2000 ). --hostmode : Use host network mode. --verbosity, -v (int): Logging verbosity (0-4, default: 3). --force, -f : Overwrite existing deployment if it exists. --dry, --dry-run : Validate and show what would be created, but do not deploy. 2. delete Delete an existing A2rchi deployment. Usage: a2rchi delete --name <deployment_name> [OPTIONS] Options: --name, -n (str): Name of the deployment to delete. --rmi : Remove container images. --rmv : Remove volumes. --keep-files : Keep deployment files (do not remove directory). --list : List all available deployments. 3. list_services List all available A2rchi services and data sources. Usage: a2rchi list_services 4. list_deployments List all existing A2rchi deployments. Usage: a2rchi list_deployments Examples Create a deployment: a2rchi create --name mybot --config configs/my.yaml --env-file secrets.env --services chat_app,uploader_app Delete a deployment and remove images/volumes: a2rchi delete --name mybot --rmi --rmv List all deployments: a2rchi list_deployments List all services: a2rchi list_services Configuration YAML API Reference The A2rchi configuration YAML file defines the deployment, services, data sources, pipelines, models, and interface settings for your A2rchi instance. Top-Level Fields name Type: string Description: Name of the deployment. global TRAINED_ON: string Description of the data or corpus the system was trained on. DATA_PATH: string Path to data storage. ACCOUNTS_PATH: string Path to user accounts. ACCEPTED_FILES: list Allowed file extensions for uploads. ROLES: list User roles available in the system. LOGGING.input_output_filename: string Log file for input/output. verbosity: int Logging verbosity (0-4). interfaces Settings for each web interface or service. chat_app , uploader_app , grader_app , grafana port: int Internal port that the Flask application binds to inside the container. This is the port the Flask server listens on within the container's network namespace. Usually don't need to change this unless you have port conflicts within the container. Default is 7861 . external_port: int External port that maps to the container's internal port, making the chat application accessible from outside the container. This is the port users will connect to in their browser (e.g., your-hostname:7861 ). When running multiple deployments on the same machine, each deployment must use a different external port to avoid conflicts. Default is 7861 . host: string Network interface address that the Flask application binds to inside the container. Setting this to 0.0.0.0 allows the application to accept connections from any network interface, which is necessary for the application to be accessible from outside the container. Shouldn't remain unchanged unless you have specific networking requirements. Default is 0.0.0.0 . hostname: string The hostname or IP address that client browsers will use to make API requests to the Flask server. This gets embedded into the JavaScript code and determines where the frontend sends its API calls. Must be set to the actual hostname/IP of the machine running the container. Using localhost will only work if accessing the application from the same machine. Default is localhost . template_folder: string Path to HTML templates. static_folder: string Path to static files (if applicable). num_responses_until_feedback: int Number of responses before the user is encouraged to provide feedback. include_copy_button: bool Show copy-to-clipboard button. enable_debug_chroma_endpoints: bool Enable debug endpoints (chat_app). flask_debug_mode: bool Enable Flask debug mode. num_problems: int Number of problems (grader_app). local_rubric_dir: string Path to rubric files (grader_app). local_users_csv_dir: string Path to users CSV (grader_app). verify_urls: bool Verify URLs on upload (uploader_app). data_manager Controls vector store, chunking, and embedding settings. collection_name: string Name of the vector collection. input_lists: list List of files with initial context URLs. local_vstore_path: string Path to local vector store. embedding_name: string Embedding backend ( OpenAIEmbeddings , HuggingFaceEmbeddings ). embedding_class_map: dict Embedding backend configuration (see below). chunk_size: int Number of characters per chunk, i.e., a string that will get embedded and stored in the vector database. Default is 1000 . chunk_overlap: int When splitting documents into chunks, how much should they overlap. Default is 0 . use_HTTP_chromadb_client: bool Use HTTP client for ChromaDB. chromadb_host: string Hostname for ChromaDB. chromadb_port: int Internal port for ChromaDB. chromadb_external_port: int Host port for ChromaDB. reset_collection: bool Reset vector collection on startup. num_documents_to_retrieve: int How many chunks to query in order of decreasing similarity (so 1 would return the most similar only, 2 the next most similar, etc.). stemming.enabled: bool Enable stemming for search. distance_metric: string Distance metric to use for similarity search in ChromaDB. Options are cosine , l2 , and ip . Read more (here)[https://docs.trychroma.com/docs/collections/configure]. Default for A2rchi is cosine. use_hybrid_search: bool Enables hybrid search, that is performing lexical search as well as semantic search. Docs retrieved from both searches are combined. The default is False bm25_weight: float Weight for BM25 in hybrid search. semantic_weight: float Weight for semantic search in hybrid search. bm25.k1: float BM25 term frequency saturation. Controls how much the score increases with additional occurrences of a term in a document. Range: [1.2,2.0] bm25.b: float BM25 length normalization. Controls how much the document length influences the score. BM25 normalizes term frequency by document length compared to the average document length in the corpus. Range: [0,1] embedding_class_map OpenAIEmbeddings: class: string kwargs.model: string similarity_score_reference: float HuggingFaceEmbeddings: class: string kwargs.model_name: string The HuggingFace embedding model you want to use. Default is sentence-transformers/all-MiniLM-L6-v2 . TODO: fix logic to require token if private model is requested. kwargs.model_kwargs.device: string ( cpu or cuda ) Argument passed to embedding model initialization, to load onto cpu (default) or cuda (GPU), which you can select if you are deploying a2rchi onto GPU. kwargs.encode_kwargs.normalize_embeddings: bool Whether to normalize the embedded vectors or not. Default is true . Note, the default distance metric that chromadb uses is l2, which measures the absolute geometric distance between vectors, so whether they are normalized or not will affect the search. similarity_score_reference: float The threshold for whether to include the link to the most relevant context in the chat response. It is an approximate distance (chromadb uses an HNSW index, where default distance function is l2 -- see more here ), so smaller values represent higher similarity. The link will be included if the score is below the chosen value. Default is 10 (scores are usually order 1, so default is to always include link). query_embedding_instructions: string or null Instructions to accompany the embedding of the query and subsequent document search. Only certain embedding models support this -- see INSTRUCTION_AWARE_MODELS in a2rchi/chains/retrievers.py to add models that support this. For example, the Qwen/Qwen3-Embedding-XB embedding models support this and are listed, see more here . Default is None . You should write the string directly into the config. An example instruction might look like: \"Given a query, retrieve relevant information to answer the query\" . You might tune it to be more specific to your use case which might improve performance. a2rchi Pipeline and model configuration. pipelines: list List of enabled pipelines (e.g., QAPipeline , GradingPipeline ). pipeline_map: dict Configuration for each pipeline: max_tokens: int prompts.required: dict Required prompt files for the pipeline. prompts.optional: dict Optional prompt files. models.required: dict Required models for the pipeline. models.optional: dict Optional models. model_class_map: dict Model backend configuration (see below). chain_update_time: int Time (seconds) between chain updates. model_class_map Each model (e.g., AnthropicLLM , OpenAIGPT4 , LlamaLLM , etc.) has: class: string kwargs: dict Model-specific parameters (see template for details). utils postgres: port: int user: string database: string host: string sso: enabled: bool sso_class: string sso_class_map: dict class: string kwargs: dict git: enabled: bool scraper: reset_data: bool verify_urls: bool enable_warnings: bool piazza: network_id: string update_time: int mattermost: update_time: int redmine: redmine_update_time: int answer_tag: string mailbox: imap4_port: int mailbox_update_time: int jira: url: string The URL of the JIRA instance from which A2rchi will fetch data. projects: list List of JIRA project names that A2rchi will fetch data from. anonymize_data: bool Boolean flag indicating whether the fetched data from JIRA should be anonymized. Default is True . anonymizer: nlp_model: string The NLP model that the spacy library will use to perform Named Entity Recognition (NER). excluded_words: list The list of words that the anonymizer should remove. greeting_patterns: list The regex patterns to match and remove greeting patterns. signoff_patterns: list The regex patterns to match and remove signoff patterns. email_pattern: string The regex pattern to match and remove email addresses. username_pattern: string The regex pattern to match and remove JIRA usernames. Required Fields Some fields are required depending on enabled services and pipelines. For example: name global.TRAINED_ON a2rchi.pipelines Service-specific fields (e.g., utils.piazza.network_id , interfaces.grader_app.num_problems ) See the User Guide for more configuration examples and explanations. Example name: my_deployment global: TRAINED_ON: \"MIT course data\" DATA_PATH: \"/root/data/\" ACCOUNTS_PATH: \"/root/.accounts/\" ACCEPTED_FILES: [\".txt\", \".pdf\"] ROLES: [\"User\", \"A2rchi\", \"Expert\"] LOGGING: input_output_filename: \"chain_input_output.log\" verbosity: 3 interfaces: chat_app: port: 7861 external_port: 7861 host: \"0.0.0.0\" hostname: \"localhost\" num_responses_until_feedback: 3 flask_debug_mode: true data_manager: collection_name: \"default_collection\" input_lists: [\"configs/miscellanea.list\"] embedding_name: \"OpenAIEmbeddings\" chunk_size: 1000 chunk_overlap: 0 distance_metric: \"cosine\" num_documents_to_retrieve: 5 a2rchi: pipelines: [\"QAPipeline\"] pipeline_map: QAPipeline: max_tokens: 10000 prompts: required: condense_prompt: \"condense.prompt\" chat_prompt: \"chat.prompt\" models: required: condense_model: \"DumbLLM\" chat_model: \"DumbLLM\" model_class_map: DumbLLM: class: DumbLLM kwargs: sleep_time_mean: 3 filler: null utils: postgres: port: 5432 user: \"a2rchi\" database: \"a2rchi-db\" host: \"postgres\" Tip: For a full template, see a2rchi/cli/templates/base-config.yaml in","title":"API Reference"},{"location":"api_reference/#api-reference","text":"","title":"API Reference"},{"location":"api_reference/#cli","text":"The A2rchi CLI provides commands to create, manage, and delete A2rchi deployments and services.","title":"CLI"},{"location":"api_reference/#commands","text":"","title":"Commands"},{"location":"api_reference/#1-create","text":"Create a new A2rchi deployment. Usage: a2rchi create --name <deployment_name> --config <config.yaml> --env-file <secrets.env> [OPTIONS] Options: --name, -n (str, required): Name of the deployment. --config, -c (str, required): Path to the YAML configuration file. --env-file, -e (str, required): Path to the secrets .env file. --services, -s (comma-separated): List of services to enable (e.g., chat_app,uploader_app ). --sources, -src (comma-separated): Data sources to enable (e.g., jira,redmine ). --podman, -p : Use Podman instead of Docker. --gpu-ids : GPU configuration ( all or comma-separated IDs). --tag, -t (str): Image tag for built containers (default: 2000 ). --hostmode : Use host network mode. --verbosity, -v (int): Logging verbosity (0-4, default: 3). --force, -f : Overwrite existing deployment if it exists. --dry, --dry-run : Validate and show what would be created, but do not deploy.","title":"1. create"},{"location":"api_reference/#2-delete","text":"Delete an existing A2rchi deployment. Usage: a2rchi delete --name <deployment_name> [OPTIONS] Options: --name, -n (str): Name of the deployment to delete. --rmi : Remove container images. --rmv : Remove volumes. --keep-files : Keep deployment files (do not remove directory). --list : List all available deployments.","title":"2. delete"},{"location":"api_reference/#3-list_services","text":"List all available A2rchi services and data sources. Usage: a2rchi list_services","title":"3. list_services"},{"location":"api_reference/#4-list_deployments","text":"List all existing A2rchi deployments. Usage: a2rchi list_deployments","title":"4. list_deployments"},{"location":"api_reference/#examples","text":"Create a deployment: a2rchi create --name mybot --config configs/my.yaml --env-file secrets.env --services chat_app,uploader_app Delete a deployment and remove images/volumes: a2rchi delete --name mybot --rmi --rmv List all deployments: a2rchi list_deployments List all services: a2rchi list_services","title":"Examples"},{"location":"api_reference/#configuration-yaml-api-reference","text":"The A2rchi configuration YAML file defines the deployment, services, data sources, pipelines, models, and interface settings for your A2rchi instance.","title":"Configuration YAML API Reference"},{"location":"api_reference/#top-level-fields","text":"","title":"Top-Level Fields"},{"location":"api_reference/#name","text":"Type: string Description: Name of the deployment.","title":"name"},{"location":"api_reference/#global","text":"TRAINED_ON: string Description of the data or corpus the system was trained on. DATA_PATH: string Path to data storage. ACCOUNTS_PATH: string Path to user accounts. ACCEPTED_FILES: list Allowed file extensions for uploads. ROLES: list User roles available in the system. LOGGING.input_output_filename: string Log file for input/output. verbosity: int Logging verbosity (0-4).","title":"global"},{"location":"api_reference/#interfaces","text":"Settings for each web interface or service.","title":"interfaces"},{"location":"api_reference/#chat_app-uploader_app-grader_app-grafana","text":"port: int Internal port that the Flask application binds to inside the container. This is the port the Flask server listens on within the container's network namespace. Usually don't need to change this unless you have port conflicts within the container. Default is 7861 . external_port: int External port that maps to the container's internal port, making the chat application accessible from outside the container. This is the port users will connect to in their browser (e.g., your-hostname:7861 ). When running multiple deployments on the same machine, each deployment must use a different external port to avoid conflicts. Default is 7861 . host: string Network interface address that the Flask application binds to inside the container. Setting this to 0.0.0.0 allows the application to accept connections from any network interface, which is necessary for the application to be accessible from outside the container. Shouldn't remain unchanged unless you have specific networking requirements. Default is 0.0.0.0 . hostname: string The hostname or IP address that client browsers will use to make API requests to the Flask server. This gets embedded into the JavaScript code and determines where the frontend sends its API calls. Must be set to the actual hostname/IP of the machine running the container. Using localhost will only work if accessing the application from the same machine. Default is localhost . template_folder: string Path to HTML templates. static_folder: string Path to static files (if applicable). num_responses_until_feedback: int Number of responses before the user is encouraged to provide feedback. include_copy_button: bool Show copy-to-clipboard button. enable_debug_chroma_endpoints: bool Enable debug endpoints (chat_app). flask_debug_mode: bool Enable Flask debug mode. num_problems: int Number of problems (grader_app). local_rubric_dir: string Path to rubric files (grader_app). local_users_csv_dir: string Path to users CSV (grader_app). verify_urls: bool Verify URLs on upload (uploader_app).","title":"chat_app, uploader_app, grader_app, grafana"},{"location":"api_reference/#data_manager","text":"Controls vector store, chunking, and embedding settings. collection_name: string Name of the vector collection. input_lists: list List of files with initial context URLs. local_vstore_path: string Path to local vector store. embedding_name: string Embedding backend ( OpenAIEmbeddings , HuggingFaceEmbeddings ). embedding_class_map: dict Embedding backend configuration (see below). chunk_size: int Number of characters per chunk, i.e., a string that will get embedded and stored in the vector database. Default is 1000 . chunk_overlap: int When splitting documents into chunks, how much should they overlap. Default is 0 . use_HTTP_chromadb_client: bool Use HTTP client for ChromaDB. chromadb_host: string Hostname for ChromaDB. chromadb_port: int Internal port for ChromaDB. chromadb_external_port: int Host port for ChromaDB. reset_collection: bool Reset vector collection on startup. num_documents_to_retrieve: int How many chunks to query in order of decreasing similarity (so 1 would return the most similar only, 2 the next most similar, etc.). stemming.enabled: bool Enable stemming for search. distance_metric: string Distance metric to use for similarity search in ChromaDB. Options are cosine , l2 , and ip . Read more (here)[https://docs.trychroma.com/docs/collections/configure]. Default for A2rchi is cosine. use_hybrid_search: bool Enables hybrid search, that is performing lexical search as well as semantic search. Docs retrieved from both searches are combined. The default is False bm25_weight: float Weight for BM25 in hybrid search. semantic_weight: float Weight for semantic search in hybrid search. bm25.k1: float BM25 term frequency saturation. Controls how much the score increases with additional occurrences of a term in a document. Range: [1.2,2.0] bm25.b: float BM25 length normalization. Controls how much the document length influences the score. BM25 normalizes term frequency by document length compared to the average document length in the corpus. Range: [0,1]","title":"data_manager"},{"location":"api_reference/#embedding_class_map","text":"OpenAIEmbeddings: class: string kwargs.model: string similarity_score_reference: float HuggingFaceEmbeddings: class: string kwargs.model_name: string The HuggingFace embedding model you want to use. Default is sentence-transformers/all-MiniLM-L6-v2 . TODO: fix logic to require token if private model is requested. kwargs.model_kwargs.device: string ( cpu or cuda ) Argument passed to embedding model initialization, to load onto cpu (default) or cuda (GPU), which you can select if you are deploying a2rchi onto GPU. kwargs.encode_kwargs.normalize_embeddings: bool Whether to normalize the embedded vectors or not. Default is true . Note, the default distance metric that chromadb uses is l2, which measures the absolute geometric distance between vectors, so whether they are normalized or not will affect the search. similarity_score_reference: float The threshold for whether to include the link to the most relevant context in the chat response. It is an approximate distance (chromadb uses an HNSW index, where default distance function is l2 -- see more here ), so smaller values represent higher similarity. The link will be included if the score is below the chosen value. Default is 10 (scores are usually order 1, so default is to always include link). query_embedding_instructions: string or null Instructions to accompany the embedding of the query and subsequent document search. Only certain embedding models support this -- see INSTRUCTION_AWARE_MODELS in a2rchi/chains/retrievers.py to add models that support this. For example, the Qwen/Qwen3-Embedding-XB embedding models support this and are listed, see more here . Default is None . You should write the string directly into the config. An example instruction might look like: \"Given a query, retrieve relevant information to answer the query\" . You might tune it to be more specific to your use case which might improve performance.","title":"embedding_class_map"},{"location":"api_reference/#a2rchi","text":"Pipeline and model configuration. pipelines: list List of enabled pipelines (e.g., QAPipeline , GradingPipeline ). pipeline_map: dict Configuration for each pipeline: max_tokens: int prompts.required: dict Required prompt files for the pipeline. prompts.optional: dict Optional prompt files. models.required: dict Required models for the pipeline. models.optional: dict Optional models. model_class_map: dict Model backend configuration (see below). chain_update_time: int Time (seconds) between chain updates.","title":"a2rchi"},{"location":"api_reference/#model_class_map","text":"Each model (e.g., AnthropicLLM , OpenAIGPT4 , LlamaLLM , etc.) has: class: string kwargs: dict Model-specific parameters (see template for details).","title":"model_class_map"},{"location":"api_reference/#utils","text":"postgres: port: int user: string database: string host: string sso: enabled: bool sso_class: string sso_class_map: dict class: string kwargs: dict git: enabled: bool scraper: reset_data: bool verify_urls: bool enable_warnings: bool piazza: network_id: string update_time: int mattermost: update_time: int redmine: redmine_update_time: int answer_tag: string mailbox: imap4_port: int mailbox_update_time: int jira: url: string The URL of the JIRA instance from which A2rchi will fetch data. projects: list List of JIRA project names that A2rchi will fetch data from. anonymize_data: bool Boolean flag indicating whether the fetched data from JIRA should be anonymized. Default is True . anonymizer: nlp_model: string The NLP model that the spacy library will use to perform Named Entity Recognition (NER). excluded_words: list The list of words that the anonymizer should remove. greeting_patterns: list The regex patterns to match and remove greeting patterns. signoff_patterns: list The regex patterns to match and remove signoff patterns. email_pattern: string The regex pattern to match and remove email addresses. username_pattern: string The regex pattern to match and remove JIRA usernames.","title":"utils"},{"location":"api_reference/#required-fields","text":"Some fields are required depending on enabled services and pipelines. For example: name global.TRAINED_ON a2rchi.pipelines Service-specific fields (e.g., utils.piazza.network_id , interfaces.grader_app.num_problems ) See the User Guide for more configuration examples and explanations.","title":"Required Fields"},{"location":"api_reference/#example","text":"name: my_deployment global: TRAINED_ON: \"MIT course data\" DATA_PATH: \"/root/data/\" ACCOUNTS_PATH: \"/root/.accounts/\" ACCEPTED_FILES: [\".txt\", \".pdf\"] ROLES: [\"User\", \"A2rchi\", \"Expert\"] LOGGING: input_output_filename: \"chain_input_output.log\" verbosity: 3 interfaces: chat_app: port: 7861 external_port: 7861 host: \"0.0.0.0\" hostname: \"localhost\" num_responses_until_feedback: 3 flask_debug_mode: true data_manager: collection_name: \"default_collection\" input_lists: [\"configs/miscellanea.list\"] embedding_name: \"OpenAIEmbeddings\" chunk_size: 1000 chunk_overlap: 0 distance_metric: \"cosine\" num_documents_to_retrieve: 5 a2rchi: pipelines: [\"QAPipeline\"] pipeline_map: QAPipeline: max_tokens: 10000 prompts: required: condense_prompt: \"condense.prompt\" chat_prompt: \"chat.prompt\" models: required: condense_model: \"DumbLLM\" chat_model: \"DumbLLM\" model_class_map: DumbLLM: class: DumbLLM kwargs: sleep_time_mean: 3 filler: null utils: postgres: port: 5432 user: \"a2rchi\" database: \"a2rchi-db\" host: \"postgres\" Tip: For a full template, see a2rchi/cli/templates/base-config.yaml in","title":"Example"},{"location":"developer_guide/","text":"Developers Guide Below is all the information which developers may need in order to get started contributing to the A2rchi project. Editing Documentation. Editing documentation requires you to install the mkdocs python packge: pip install mkdocs To edit documentation, simply make changes to the .md and .yml files in the ./docs folder. To view your changes without pushing them, cd into the ./docs folder and then run mkdocs serve . Add the -a IP:HOST argument (default is localhost:8000) to specify where to host the docs so you can easily view your changes on the web. To publish your changes, run mkdocs gh-deploy . Please make sure to also open a PR to merge the documentation changes into main. Note, please do NOT edit files in the gh-pages branch by hand, again, make a PR to main from a separate branch, and then you can deploy from main with the new changes. DockerHub Images A2rchi will load from different base images hosted on dockerhub. If you do not need to use GPUs the python base image will be installed. Alternatively, the pytorch base image will be installed. The base Docker file used to make these base images from which the chat interface inherits its changes from can be found in a2rchi/cli/templates/dockerfiles/base-X-image directories. They are currently hosted on dockerhub at the following links: pytorch: https://hub.docker.com/r/a2rchi/a2rchi-pytorch-base python: https://hub.docker.com/r/a2rchi/a2rchi-python-base In order to rebuild the base images from which the dockerfiles inherit, go to the base-xxx-image directory found in templates/dockerfiles/ . In these directories, there is a different set of requirements for each along with their license and respective dockerfiles. To regenerate the requirements if they have been changed run the following commands to ensure that the right header is used for each: for the python image: cat requirements/cpu-requirementsHEADER.txt requirements/requirements-base.txt > a2rchi/templates/dockerfiles/base-python-image/requirements.txt for the pytorch image: cat requirements/gpu-requirementsHEADER.txt requirements/requirements-base.txt > a2rchi/templates/dockerfiles/base-python-image/requirements.txt Then while inside of the templates/dockerfiles/base-xxx-image directory, simply run the following command to build the image. podman build -t a2rchi/<image-name>:<tag> . after having checked that the newly built image is working, to update it on dockerhub, login to dockerhub using (ask for a senior developer for the password/master token), podman login docker.io and finally push the image to the repository as such: podman push a2rchi/<image-name>:<tag>","title":"Developer Guide"},{"location":"developer_guide/#developers-guide","text":"Below is all the information which developers may need in order to get started contributing to the A2rchi project.","title":"Developers Guide"},{"location":"developer_guide/#editing-documentation","text":"Editing documentation requires you to install the mkdocs python packge: pip install mkdocs To edit documentation, simply make changes to the .md and .yml files in the ./docs folder. To view your changes without pushing them, cd into the ./docs folder and then run mkdocs serve . Add the -a IP:HOST argument (default is localhost:8000) to specify where to host the docs so you can easily view your changes on the web. To publish your changes, run mkdocs gh-deploy . Please make sure to also open a PR to merge the documentation changes into main. Note, please do NOT edit files in the gh-pages branch by hand, again, make a PR to main from a separate branch, and then you can deploy from main with the new changes.","title":"Editing Documentation."},{"location":"developer_guide/#dockerhub-images","text":"A2rchi will load from different base images hosted on dockerhub. If you do not need to use GPUs the python base image will be installed. Alternatively, the pytorch base image will be installed. The base Docker file used to make these base images from which the chat interface inherits its changes from can be found in a2rchi/cli/templates/dockerfiles/base-X-image directories. They are currently hosted on dockerhub at the following links: pytorch: https://hub.docker.com/r/a2rchi/a2rchi-pytorch-base python: https://hub.docker.com/r/a2rchi/a2rchi-python-base In order to rebuild the base images from which the dockerfiles inherit, go to the base-xxx-image directory found in templates/dockerfiles/ . In these directories, there is a different set of requirements for each along with their license and respective dockerfiles. To regenerate the requirements if they have been changed run the following commands to ensure that the right header is used for each: for the python image: cat requirements/cpu-requirementsHEADER.txt requirements/requirements-base.txt > a2rchi/templates/dockerfiles/base-python-image/requirements.txt for the pytorch image: cat requirements/gpu-requirementsHEADER.txt requirements/requirements-base.txt > a2rchi/templates/dockerfiles/base-python-image/requirements.txt Then while inside of the templates/dockerfiles/base-xxx-image directory, simply run the following command to build the image. podman build -t a2rchi/<image-name>:<tag> . after having checked that the newly built image is working, to update it on dockerhub, login to dockerhub using (ask for a senior developer for the password/master token), podman login docker.io and finally push the image to the repository as such: podman push a2rchi/<image-name>:<tag>","title":"DockerHub Images"},{"location":"install/","text":"Install System Requirements A2rchi is deployed using a python-based CLI onto containers. It requires: docker version 24+ or podman version 5.4.0+ (for containers) python 3.10.0+ (for CLI) Note: We support either running open source models locally, or connecting to existing APIs. If you plan to run open source models on your machine's GPUs, please check out the Advanced Setup & Deployment section for more information. Install Clone the a2rchi repo: git clone https://github.com/mit-submit/A2rchi.git From the repository run: pip install -e . This will install A2rchi's dependencies as well as a local CLI tool. You should be able to see that it is installed with which a2rchi which will show the full path of the a2rchi executable. Show Full Installation Script You can use the following script to set up A2rchi from scratch. Copy and paste it into your terminal: # Clone the repository git clone https://github.com/mit-submit/A2rchi.git cd A2rchi export A2RCHI_DIR=$(pwd) # (Optional) Create and activate a virtual environment python3 -m venv .a2rchi_venv source .a2rchi_venv/bin/activate # Install dependencies cd $A2RCHI_DIR pip install -e . # Verify installation which a2rchi","title":"Install"},{"location":"install/#install","text":"","title":"Install"},{"location":"install/#system-requirements","text":"A2rchi is deployed using a python-based CLI onto containers. It requires: docker version 24+ or podman version 5.4.0+ (for containers) python 3.10.0+ (for CLI) Note: We support either running open source models locally, or connecting to existing APIs. If you plan to run open source models on your machine's GPUs, please check out the Advanced Setup & Deployment section for more information.","title":"System Requirements"},{"location":"install/#install_1","text":"Clone the a2rchi repo: git clone https://github.com/mit-submit/A2rchi.git From the repository run: pip install -e . This will install A2rchi's dependencies as well as a local CLI tool. You should be able to see that it is installed with which a2rchi which will show the full path of the a2rchi executable. Show Full Installation Script You can use the following script to set up A2rchi from scratch. Copy and paste it into your terminal: # Clone the repository git clone https://github.com/mit-submit/A2rchi.git cd A2rchi export A2RCHI_DIR=$(pwd) # (Optional) Create and activate a virtual environment python3 -m venv .a2rchi_venv source .a2rchi_venv/bin/activate # Install dependencies cd $A2RCHI_DIR pip install -e . # Verify installation which a2rchi","title":"Install"},{"location":"quickstart/","text":"Quickstart Deploy your first instance of A2rchi and walk through the important concepts. Sources and Services A2rchi can ingest data from a variety of sources and supports several services . List them with the CLI command, and decide which ones you want to use, so that we can configure them. $ a2rchi list-services Available A2RCHI services: Application Services: chatbot Interactive chat interface for users to communicate with the AI agent grafana Monitoring dashboard for system and LLM performance metrics uploader Admin interface for uploading and managing documents grader Automated grading service for assignments with web interface Integration Services: piazza Integration service for Piazza posts and Slack notifications mattermost Integration service for Mattermost channels redmine-mailer Email processing and Cleo/Redmine ticket management Data Sources: redmine Redmine issue tracking integration jira Jira issue tracking integration See the User Guide for detailed information about each service and source. Pipelines A2rchi supports several pipelines, which are pre-defined sequences of operations that process user inputs and generate responses. A particular service will support a subset of the pipelines, see the User Guide for more details. An example pipeline is the QAPipeline , which is a question-answering pipeline that takes a user's question, retrieves relevant documents from the vector store, and generates an answer using a language model. We will specify which pipelines we are interested in making available to A2rchi in the configuration file. Configuration Once you have chosen the services, sources, and pipelines you want to use, you can create a configuration file which specifies these and their settings. You can start from one of the example configuration files in configs/ , or create your own from scratch. The services and sources that A2rchi will use are not determined here, only their parameters are being set. Important: The configuration file follows the format of a2rchi/templates/base-config.yaml , and any fields not specified in your configuration file will be filled in with the defaults from this base config. Here is an example configuration file which configures some specific settings for the chatbot service using the QAPipeline pipeline with a local VLLM model. Save it as configs/my_config.yaml : name: my_a2rchi global: TRAINED_ON: \"My data\" data_manager: input_lists: - configs/miscellanea.list embedding_name: HuggingFaceEmbeddings chromadb_host: localhost a2rchi: pipelines: - QAPipeline pipeline_map: QAPipeline: prompts: required: condense_prompt: configs/prompts/condense.prompt chat_prompt: configs/prompts/submit.prompt models: required: chat_model: VLLM condense_model: VLLM model_class_map: VLLM: kwargs: base_model: deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B interfaces: chat_app: HOSTNAME: \"<your-hostname>\" Explanation of config parameters Here is a brief explanation of the parameters in the example configuration file: name : The name of your A2rchi deployment. global:TRAINED_ON : A brief description of the documents you are uploading to A2rchi. data_manager : Settings related to data management, including: input_lists : A list of files containing links to be ingested. embedding_name : The embedding model to use for vectorization. chromadb_host : The host where ChromaDB is running. a2rchi : Settings related to the A2rchi core, including: pipelines : The pipelines to use (e.g., QAPipeline ). pipeline_map : Configuration for each pipeline, including prompts and models. model_class_map : Mapping of model names to their classes and parameters. interfaces : Settings for the services/interfaces, including: chat_app : Configuration for the chat application, including the hostname. Secrets Secrets are values which are sensitive and therefore should not be directly included in code or configuration files. They typically include passwords, API keys, etc. To manage these secrets, we ask that you write them to a location on your file system in a single .env file. The only secret that is required to launch a minimal version of A2rchi (chatbot with open source LLM and embeddings) is: PG_PASSWORD : some password you pick which encrypts the database. So for a basic deployment, all you need is to create the 'secrets' file is: echo \"PG_PASSWORD=my_strong_password\" > ~/.secrets.env If you are not running an open source model, you can use various OpenAI or Anthropic models if you provide the following secrets, openai_api_key : the API key given by OpenAI anthropic_api_key : the API key given by Anthropic respectively. If you want to access private embedding models or LLMs from HuggingFace, you will need to provide the following: hf_token : the API key to your HuggingFace account For many of the other services provided by A2rchi, additional secrets are required. These are detailed in User's Guide Creating an A2rchi Deployment Create your deployment with the CLI: a2rchi create --name my-a2rchi --config configs/my_config.yaml --podman -e .secrets.env --services chatbot Here we specify: --name : the name of your deployment --config : the path to your configuration file --podman : use podman for container management ( docker is default) -e : the path to your secrets --services : the services to deploy (here we only deploy the chatbot service) Output Example $ a2rchi create --name my-a2rchi -c test.yaml --podman -e secrets.env --services chatbot Starting A2RCHI deployment process... [a2rchi] Creating deployment 'my-a2rchi' with services: chatbot [a2rchi] Auto-enabling dependencies: postgres, chromadb [a2rchi] Configuration validated successfully [a2rchi] You are using an embedding model from HuggingFace; make sure to include a HuggingFace token if required for usage, it won't be explicitly enforced [a2rchi] Required secrets validated: PG_PASSWORD [a2rchi] Volume 'a2rchi-pg-my-a2rchi' already exists. No action needed. [a2rchi] Volume 'a2rchi-my-a2rchi' already exists. No action needed. [a2rchi] Starting compose deployment from /path/to/my/.a2rchi/a2rchi-my-a2rchi [a2rchi] Using compose file: /path/to/my/.a2rchi/a2rchi-my-a2rchi/compose.yaml [a2rchi] (This might take a minute...) [a2rchi] Deployment started successfully A2RCHI deployment 'my-a2rchi' created successfully! Services running: chatbot, postgres, chromadb [a2rchi] Chatbot: http://localhost:7861 The first time you run this command it will take longer than usual (order minutes) because docker / podman will have to build the container images from scratch; subsequent deployments which will re-use the images will be quicker (around a minute). Note: having issues? Run the command with the -v 4 flag to enable DEBUG level logging. Verifying a deployment Check that a deployment is running with the A2rchi CLI: a2rchi list-deployments You should see something like: Existing deployments: my-a2rchi You can also verify that all your images are up and running properly in containers by executing the command: podman ps You should see something like: CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 7e823e15e8d8 localhost/chromadb-my-a2rchi:2000 uvicorn chromadb.... About a minute ago Up About a minute (healthy) 0.0.0.0:8010->8000/tcp chromadb-my-a2rchi 8d561db18278 docker.io/library/postgres:16 postgres About a minute ago Up About a minute (healthy) 5432/tcp postgres-my-a2rchi a1f7f9b44b1d localhost/chat-my-a2rchi:2000 python -u a2rchi/... About a minute ago Up About a minute 0.0.0.0:7868->7868/tcp chat-my-a2rchi To access the chat interface, visit its corresponding port ( 0.0.0.0:7868 in the above example). Removing a deployment Lastly, to tear down the deployment, simply run: a2rchi delete --name my-a2rchi","title":"Quickstart"},{"location":"quickstart/#quickstart","text":"Deploy your first instance of A2rchi and walk through the important concepts.","title":"Quickstart"},{"location":"quickstart/#sources-and-services","text":"A2rchi can ingest data from a variety of sources and supports several services . List them with the CLI command, and decide which ones you want to use, so that we can configure them. $ a2rchi list-services Available A2RCHI services: Application Services: chatbot Interactive chat interface for users to communicate with the AI agent grafana Monitoring dashboard for system and LLM performance metrics uploader Admin interface for uploading and managing documents grader Automated grading service for assignments with web interface Integration Services: piazza Integration service for Piazza posts and Slack notifications mattermost Integration service for Mattermost channels redmine-mailer Email processing and Cleo/Redmine ticket management Data Sources: redmine Redmine issue tracking integration jira Jira issue tracking integration See the User Guide for detailed information about each service and source.","title":"Sources and Services"},{"location":"quickstart/#pipelines","text":"A2rchi supports several pipelines, which are pre-defined sequences of operations that process user inputs and generate responses. A particular service will support a subset of the pipelines, see the User Guide for more details. An example pipeline is the QAPipeline , which is a question-answering pipeline that takes a user's question, retrieves relevant documents from the vector store, and generates an answer using a language model. We will specify which pipelines we are interested in making available to A2rchi in the configuration file.","title":"Pipelines"},{"location":"quickstart/#configuration","text":"Once you have chosen the services, sources, and pipelines you want to use, you can create a configuration file which specifies these and their settings. You can start from one of the example configuration files in configs/ , or create your own from scratch. The services and sources that A2rchi will use are not determined here, only their parameters are being set. Important: The configuration file follows the format of a2rchi/templates/base-config.yaml , and any fields not specified in your configuration file will be filled in with the defaults from this base config. Here is an example configuration file which configures some specific settings for the chatbot service using the QAPipeline pipeline with a local VLLM model. Save it as configs/my_config.yaml : name: my_a2rchi global: TRAINED_ON: \"My data\" data_manager: input_lists: - configs/miscellanea.list embedding_name: HuggingFaceEmbeddings chromadb_host: localhost a2rchi: pipelines: - QAPipeline pipeline_map: QAPipeline: prompts: required: condense_prompt: configs/prompts/condense.prompt chat_prompt: configs/prompts/submit.prompt models: required: chat_model: VLLM condense_model: VLLM model_class_map: VLLM: kwargs: base_model: deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B interfaces: chat_app: HOSTNAME: \"<your-hostname>\" Explanation of config parameters Here is a brief explanation of the parameters in the example configuration file: name : The name of your A2rchi deployment. global:TRAINED_ON : A brief description of the documents you are uploading to A2rchi. data_manager : Settings related to data management, including: input_lists : A list of files containing links to be ingested. embedding_name : The embedding model to use for vectorization. chromadb_host : The host where ChromaDB is running. a2rchi : Settings related to the A2rchi core, including: pipelines : The pipelines to use (e.g., QAPipeline ). pipeline_map : Configuration for each pipeline, including prompts and models. model_class_map : Mapping of model names to their classes and parameters. interfaces : Settings for the services/interfaces, including: chat_app : Configuration for the chat application, including the hostname.","title":"Configuration"},{"location":"quickstart/#secrets","text":"Secrets are values which are sensitive and therefore should not be directly included in code or configuration files. They typically include passwords, API keys, etc. To manage these secrets, we ask that you write them to a location on your file system in a single .env file. The only secret that is required to launch a minimal version of A2rchi (chatbot with open source LLM and embeddings) is: PG_PASSWORD : some password you pick which encrypts the database. So for a basic deployment, all you need is to create the 'secrets' file is: echo \"PG_PASSWORD=my_strong_password\" > ~/.secrets.env If you are not running an open source model, you can use various OpenAI or Anthropic models if you provide the following secrets, openai_api_key : the API key given by OpenAI anthropic_api_key : the API key given by Anthropic respectively. If you want to access private embedding models or LLMs from HuggingFace, you will need to provide the following: hf_token : the API key to your HuggingFace account For many of the other services provided by A2rchi, additional secrets are required. These are detailed in User's Guide","title":"Secrets"},{"location":"quickstart/#creating-an-a2rchi-deployment","text":"Create your deployment with the CLI: a2rchi create --name my-a2rchi --config configs/my_config.yaml --podman -e .secrets.env --services chatbot Here we specify: --name : the name of your deployment --config : the path to your configuration file --podman : use podman for container management ( docker is default) -e : the path to your secrets --services : the services to deploy (here we only deploy the chatbot service) Output Example $ a2rchi create --name my-a2rchi -c test.yaml --podman -e secrets.env --services chatbot Starting A2RCHI deployment process... [a2rchi] Creating deployment 'my-a2rchi' with services: chatbot [a2rchi] Auto-enabling dependencies: postgres, chromadb [a2rchi] Configuration validated successfully [a2rchi] You are using an embedding model from HuggingFace; make sure to include a HuggingFace token if required for usage, it won't be explicitly enforced [a2rchi] Required secrets validated: PG_PASSWORD [a2rchi] Volume 'a2rchi-pg-my-a2rchi' already exists. No action needed. [a2rchi] Volume 'a2rchi-my-a2rchi' already exists. No action needed. [a2rchi] Starting compose deployment from /path/to/my/.a2rchi/a2rchi-my-a2rchi [a2rchi] Using compose file: /path/to/my/.a2rchi/a2rchi-my-a2rchi/compose.yaml [a2rchi] (This might take a minute...) [a2rchi] Deployment started successfully A2RCHI deployment 'my-a2rchi' created successfully! Services running: chatbot, postgres, chromadb [a2rchi] Chatbot: http://localhost:7861 The first time you run this command it will take longer than usual (order minutes) because docker / podman will have to build the container images from scratch; subsequent deployments which will re-use the images will be quicker (around a minute). Note: having issues? Run the command with the -v 4 flag to enable DEBUG level logging.","title":"Creating an A2rchi Deployment"},{"location":"quickstart/#verifying-a-deployment","text":"Check that a deployment is running with the A2rchi CLI: a2rchi list-deployments You should see something like: Existing deployments: my-a2rchi You can also verify that all your images are up and running properly in containers by executing the command: podman ps You should see something like: CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 7e823e15e8d8 localhost/chromadb-my-a2rchi:2000 uvicorn chromadb.... About a minute ago Up About a minute (healthy) 0.0.0.0:8010->8000/tcp chromadb-my-a2rchi 8d561db18278 docker.io/library/postgres:16 postgres About a minute ago Up About a minute (healthy) 5432/tcp postgres-my-a2rchi a1f7f9b44b1d localhost/chat-my-a2rchi:2000 python -u a2rchi/... About a minute ago Up About a minute 0.0.0.0:7868->7868/tcp chat-my-a2rchi To access the chat interface, visit its corresponding port ( 0.0.0.0:7868 in the above example).","title":"Verifying a deployment"},{"location":"quickstart/#removing-a-deployment","text":"Lastly, to tear down the deployment, simply run: a2rchi delete --name my-a2rchi","title":"Removing a deployment"},{"location":"user_guide/","text":"User Guide Overview A2rchi supports various data sources as easy ways to ingest your data into the vector store databased used for document retrieval. These include: Links lists (even behind SSO) : automatically scrape and ingest documents from a list of URLs Git scraping : git mkdocs repositories Ticketing systems : JIRA, Redmine, Piazza Local documents Additionally, A2rchi supports various interfaces/services , which are applications that interact with the RAG system. These include: Chat interface : a web-based chat application Piazza integration : read posts from Piazza and post draft responses to a Slack channel Cleo/Redmine integration : read emails and create tickets in Redmine Mattermost integration : read posts from Mattermost and post draft responses to a Mattermost channel Grafana monitoring dashboard : monitor system and LLM performance metrics Document uploader : web interface for uploading and managing documents Grader : automated grading service for assignments with web interface Both data sources and interfaces/services are enabled via flags to the a2rchi create command, a2rchi create [...] --services=chatbot,piazza,jira,... The parameters of the services are configured via the configuration file. See below for more details. We support various pipelines which are pre-defined sequences of operations that process user inputs and generate responses. Each service may support a given pipeline. See the Services and Pipelines sections below for more details. For each pipeline, you can use different models, retrievers, and prompts for different steps of the pipeline. We support various models for both embeddings and LLMs, which can be run locally or accessed via APIs. See the Models section below for more details. Both pipelines and models are configured via the configuration file. Finally, we support various retrievers and embedding techniques for document retrieval. These are configured via the configuration file. See the Vector Store section below for more details. Data Sources These are the different ways to ingest data into the vector store used for document retrieval. WEb Link Lists A web link list is a simple text file containing a list of URLs, one per line. A2rchi will fetch the content from each URL and add it to the vector store, using the Scraper class. Configuration You can define which lists of links A2rchi will inject in the configuration file as follows: data_manager: input_lists: # REQUIRED - configs/miscellanea.list # list of websites with relevant info - [...other lists...] Each list should be a simple text file containing one URL per line, e.g., https://example.com/page1 https://example.com/page2 [...] In the case that some of the links are behind a Single Sign-On (SSO) system, you can use the SSOScraper . To enable it, add the enable it and pick the class you want in the configuration file: utils: sso: sso_class: CERNSSO # or whichever class you want to use enabled: true Secrets If you are using SSO, depending on the class, you may need to provide your login credentials in a secrets file as follows: SSO_USERNAME=username SSO_PASSWORD=password Then, make sure that the links you provide in the .list file(s) start with sso- , e.g., sso-https://example.com/protected/page Running Link scraping is automatically enabled in A2rchi, you don't need to add any arguments to the create command. Git scraping In some cases, the RAG input may be documentations based on MKDocs git repositories. Instead of scraping these sites as regular HTML sites you can obtain the relevant content using the GitScraper class. Configuration To configure it, simply add the following field in the configuration file: utils: git: enabled: {{ utils.git.enabled | default(false, true) }} In the input lists, make sure to prepend git- to the URL of the repositories you are interested in scraping. git-https://gitlab.cern.ch/cms-tier0-ops/documentation.git Secrets You will need to provide a git username and token in the secrets file, GIT_USERNAME=your_username GIT_TOKEN=your_token Running Git link scraping is automatically enabled in A2rchi once enabled in the config, you don't need to add any arguments to the create command. JIRA The JIRA integration allows A2rchi to fetch issues and comments from specified JIRA projects and add them to the vector store, using the JiraScraper class. Configuration Select which projects to scrape in the configuration file: jira: url: {{ utils.jira.JIRA_URL }} projects: {%- for project in utils.jira.JIRA_PROJECTS %} - {{ project }} {%- endfor %} anonymize_data: {{ utils.jira.ANONYMIZE_DATA | default(true, true) }} You can turn on an automatic anonymizer of the data fetched from JIRA via the anonymize_data config. anonymizer: nlp_model: {{ utils.anonymizer.nlp_model | default('en_core_web_sm', true) }} excluded_words: {%- for word in utils.anonymizer.excluded_words | default(['John', 'Jane', 'Doe']) %} - {{ word }} {%- endfor %} greeting_patterns: {%- for pattern in utils.anonymizer.greeting_patterns | default(['^(hi|hello|hey|greetings|dear)\\\\b', '^\\\\w+,\\\\s*']) %} - {{ pattern }} {%- endfor %} signoff_patterns: {%- for pattern in utils.anonymizer.signoff_patterns | default(['\\\\b(regards|sincerely|best regards|cheers|thank you)\\\\b', '^\\\\s*[-~]+\\\\s*$']) %} - {{ pattern }} {%- endfor %} email_pattern: '{{ utils.anonymizer.email_pattern | default(\"[\\\\w\\\\.-]+@[\\\\w\\\\.-]+\\\\.\\\\w+\") }}' username_pattern: '{{ utils.anonymizer.username_pattern | default(\"\\\\[~[^\\\\]]+\\\\]\") }}' The anonymizer will remove names, emails, usernames, greetings, signoffs, and any other words you specify from the fetched data. This is useful if you want to avoid having personal information in the vector store. Secrets A personal access token (PAT) is required to authenticate and authorize with JIRA. This token should be placed in a secrets file as JIRA_PAT . Running To enable JIRA scraping, run with, a2rchi create [...] --services=jira Adding Documents and the Uploader Interface Adding Documents There are two main ways to add documents to A2rchi's vector database. They are: Manually adding files while the service is running via the uploader GUI Directly copying files into the container These methods are outlined below. Manual Uploader In order to upload documents while A2rchi is running via an easily accessible GUI, use the upload-manager built into the system. The manager is run as an additional docker service by adding the following argument to the CLI command: a2rchi create [...] --services=uploader The exact port may vary based on configuration (default is 5001 ). A simple docker ps -a command run on the server will inform which port it's being run on. In order to access the manager, one must first make an account. To do this, first get the ID or name of the uploader container using docker ps -a . Then, accese the container using docker exec -it <CONTAINER-ID> bash so you can run python bin/service_create_account.py from the /root/A2rchi/a2rchi directory.\u00b7 This script will guide you through creating an account. Note that we do not guarantee the security of this account, so never upload critical passwords to create it. Once you have created an account, visit the outgoing port of the data manager docker service and then log in. The GUI will then allow you to upload documents while A2rchi is still running. Note that it may take a few minutes for all the documents to upload. Directly copying files to the container The documents used for RAG live in the chat container at /root/data/<directory>/<files> . Thus, in a pinch, you can docker/podman cp a file at this directory level, e.g., podman/docker cp myfile.pdf <container name or ID>:/root/data/<new_dir>/ . If you need to make a new directory in the container, you can do podman exec -it <container name or ID> mkdir /root/data/<new_dir> . Redmine Input from Redmine tickets as a data source. Secrets REDMINE_URL REDMINE_USER REDMINE_PW REDMINE_PROJECT Running a2rchi create [...] --services=redmine Interfaces/Services These are the different apps that A2rchi supports, which allow you to interact with the AI pipelines. Piazza Interface Set up A2rchi to read posts from your Piazza forum and post draft responses to a specified Slack channel. To do this, a Piazza login (email and password) is required, plus the network ID of your Piazza channel, and lastly, a Webhook for the slack channel A2rchi will post to. See below for a step-by-step description of this. Go to https://api.slack.com/apps and sign in to workspace where you will eventually want A2rchi to post to (note doing this in a business workspace like the MIT one will require approval of the app/bot). Click 'Create New App', and then 'From scratch'. Name your app and again select the correct workspace. Then hit 'Create App' Now you have your app, and there are a few things to configure before you can launch A2rchi: Go to Incoming Webhooks under Features, and toggle it on. Click 'Add New Webhook', and select the channel you want A2rchi to post to. Now, copy the 'Webhook URL' and paste it into the secrets file, and handle it like any other secret! Configuration Beyond standard required configuration fields, the network ID of the Piazza channel is required (see below for an example config). You can get the network ID by simply navigating to the class homepage, and grabbing the sequence that follows 'https://piazza.com/class/'. For example, the 8.01 Fall 2024 homepage is: 'https://piazza.com/class/m0g3v0ahsqm2lg'. The network ID is thus 'm0g3v0ahsqm2lg'. Example minimal config for the Piazza interface: name: bare_minimum_configuration #REQUIRED global: TRAINED_ON: \"Your class materials\" #REQUIRED chains: input_lists: #REQUIRED - configs/class_info.list # list of websites with class info a2rchi: [... a2rchi config ...] utils: piazza: network_id: <your Piazza network ID here> # REQUIRED Secrets The necessary secrets for deploying the Piazza service are the following: PIAZZA_EMAIL=... PIAZZA_PASSWORD=... SLACK_WEBHOOK=... The Slack webhook secret is described above. The Piazza email and password should be those of one of the class instructors. Remember to put this information in files named following what is written above. Running To run the Piazza service, simply add the piazza flag. For example: a2rchi create [...] --services=piazza Redmine/Mailbox Interface A2rchi will read all new tickets in a Redmine project, and draft a response as a comment to the ticket. Once the ticket is updated to the \"Resolved\" status by an admin, A2rchi will send the response as an email to the user who opened the ticket. The admin can modify A2rchi's response before sending it out. Configuration redmine: redmine_update_time: {{ utils.redmine.redmine_update_time | default(10, true) }} answer_tag: {{ utils.redmine.answer_tag | default('-- A2rchi -- Resolving email was sent', true) }} Secrets required_secrets=['IMAP_USER', 'IMAP_PW', 'REDMINE_URL', 'REDMINE_USER', 'REDMINE_PW', 'REDMINE_PROJECT', 'SENDER_SERVER', 'SENDER_PORT', 'SENDER_REPLYTO', 'SENDER_USER', 'SENDER_PW'] Running a2rchi create [...] --services=redmine-mailer Mattermost Interface Set up A2rchi to read posts from your Mattermost forum and post draft responses to a specified Mattermost channel. Configuration mattermost: update_time: {{ utils.mattermost.update_time | default(60, true) }} Secrets You need to specify a webhook, a key and the id of two channels to read and write. Should be specified like this. MATTERMOST_WEBHOOK=... MATTERMOST_KEY=... MATTERMOST_CHANNEL_ID_READ=... MATTERMOST_CHANNEL_ID_WRITE=... Running To run the Mattermost service, simply add the Mattermost to the services flag. For example: a2rchi create [...] --services=mattermost Grafana Interface Monitor the performance of your A2rchi instance with the Grafana interface. This service provides a web-based dashboard to visualize various metrics related to system performance, LLM usage, and more. Note, if you are deploying a version of A2rchi you have already used (i.e., you haven't removed the images/volumes for a given --name ), the postgres will have already been created without the Grafana user created, and it will not work, so make sure to deploy a fresh instance. Configuration grafana: external_port: {{ interfaces.grafana.external_port | default(3000, true) }} Secrets To run the Grafana service, you first need to specify a password for the Grafana to access the postgres database that stores the information. Set the environment variable as follows in the secrets file: PG_PASSWORD=<your_password> Running Once this is set, add the following argument to your a2rchi create command, e.g., a2rchi create [...] --services=grafana and you should see something like this CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES d27482864238 localhost/chromadb-gtesting2:2000 uvicorn chromadb.... 9 minutes ago Up 9 minutes (healthy) 0.0.0.0:8000->8000/tcp, 8000/tcp chromadb-gtesting2 87f1c7289d29 docker.io/library/postgres:16 postgres 9 minutes ago Up 9 minutes (healthy) 5432/tcp postgres-gtesting2 40130e8e23de docker.io/library/grafana-gtesting2:2000 9 minutes ago Up 9 minutes 0.0.0.0:3000->3000/tcp, 3000/tcp grafana-gtesting2 d6ce8a149439 localhost/chat-gtesting2:2000 python -u a2rchi/... 9 minutes ago Up 9 minutes 0.0.0.0:7861->7861/tcp chat-gtesting2 where the grafana interface is accessible at your-hostname:3000 . To change the external port from 3000 , you can do this in the config at interfaces:grafana:EXTERNAL_PORT . The default login and password are both \"admin\", which you will be prompted to change should you want to after first logging in. Navigate to the A2rchi dashboard from the home page by going to the menu > Dashboards > A2rchi > A2rchi Usage. Note, your-hostname here is the just name of the machine. Grafana uses its default configuration which is localhost but unlike the chat interface, there are no APIs where we template with a selected hostname, so the container networking handles this nicely. Pro tip: once at the web interface, for the \"Recent Conversation Messages (Clean Text + Link)\" panel, click the three little dots in the top right hand corner of the panel, click \"Edit\", and on the right, go to e.g., \"Override 4\" (should have Fields with name: clean text, also Override 7 for context column) and override property \"Cell options > Cell value inspect\". This will allow you to expand the text boxes with messages longer than can fit. Make sure you click apply to keep the changes. Pro tip 2: If you want to download all of the information from any panel as a CSV, go to the same three dots and click \"Inspect\", and you should see the option. Grader Interface Interface to launch a website which for a provided solution and rubric (and a couple of other things detailed below), will grade scanned images of a handwritten solution for the specified problem(s). Nota bene: this is not yet fully generalized and \"service\" ready, but instead for testing grading pipelines and a base off of which to build a potential grading app. Requirements To launch the service the following files are required: users.csv . This file is .csv file that contains two columns: \"MIT email\" and \"Unique code\", e.g.: MIT email,Unique code username@mit.edu,222 For now, the system requires the emails to be in the MIT domain, namely, contain \"@mit.edu\". TODO: make this an argument that is passed (e.g., school/email domain) solution_with_rubric_*.txt . These are .txt files that contain the problem solution followed by the rubric. The naming of the files should follow exactly, where the * is the problem number. There should be one of these files for every problem you want the app to be able to grade. The top of the file should be the problem name with a line of dashes (\"-\") below, e.g.: Anti-Helmholtz Coils --------------------------------------------------- These files should live in a directory which you will pass to the config, and A2rchi will handle the rest. admin_password.txt . This file will be passed as a secret and be the admin code to login in to the page where you can reset attempts for students. Secrets The only grading specific secret is the admin password, which like shown above, should be put in the following file ADMIN_PASSWORD=your_password Then it behaves like any other secret. Configuration The required fields in the configuration file are different from the rest of the A2rchi services. Below is an example: name: grading_test # REQUIRED global: TRAINED_ON: \"rubrics, class info, etc.\" # REQUIRED a2rchi: pipelines: - GradingPipeline pipeline_map: GradingPipeline: prompts: required: final_grade_prompt: configs/prompts/final_grade.prompt models: required: final_grade_model: OllamaInterface ImageProcessingPipeline: prompts: required: image_processing_prompt: configs/prompts/image_processing.prompt models: required: image_processing_model: OllamaInterface interfaces: grader_app: num_problems: 1 # REQUIRED local_rubric_dir: ~/grading/my_rubrics # REQUIRED local_users_csv_dir: ~/grading/logins # REQUIRED data_manager: [...] name -- The name of your configuration (required). global.TRAINED_ON -- A brief description of the data or materials A2rchi is trained on (required). a2rchi.pipelines -- List of pipelines to use (e.g., GradingPipeline , ImageProcessingPipeline ). a2rchi.pipeline_map -- Mapping of pipelines to their required prompts and models. a2rchi.pipeline_map.GradingPipeline.prompts.required.final_grade_prompt -- Path to the grading prompt file for evaluating student solutions. a2rchi.pipeline_map.GradingPipeline.models.required.final_grade_model -- Model class for grading (e.g., OllamaInterface , HuggingFaceOpenLLM ). a2rchi.pipeline_map.ImageProcessingPipeline.prompts.required.image_processing_prompt -- Path to the prompt file for image processing. a2rchi.pipeline_map.ImageProcessingPipeline.models.required.image_processing_model -- Model class for image processing (e.g., OllamaInterface , HuggingFaceImageLLM ). interfaces.grader_app.num_problems -- Number of problems the grading service should expect (must match the number of rubric files). interfaces.grader_app.local_rubric_dir -- Directory containing the solution_with_rubric_*.txt files. interfaces.grader_app.local_users_csv_dir -- Directory containing the users.csv file. Running a2rchi create [...] --services=grader Models Models are either: Hosted locally, either via VLLM or HuggingFace transformers. Accessed via an API, e.g., OpenAI, Anthropic, etc. Accessed via an Ollama server instance. Local Models To use a local model, specify one of the local model classes in models.py : HuggingFaceOpenLLM HuggingFaceImageLLM VLLM Models via APIs We support the following model classes in models.py for models accessed via APIs: OpenAILLM ClaudeLLM AnthropicLLM Ollama In order to use an Ollama server instance for the chatbot, it is possible to specify OllamaInterface for the model name. To then correctly use models on the Ollama server, in the keyword args, specify both the url of the server and the name of a model hosted on the server. a2rchi: chain: model_class_map: OllamaInterface: kwargs: base_model: \"gemma3\" # example url: \"url-for-server\" In this case, the gemma3 model is hosted on the Ollama server at url-for-server . You can check which models are hosted on your server by going to url-for-server/models . Other Some useful additional features supported by the framework. Add ChromaDB Document Management API Endpoints Debugging ChromaDB endpoints Debugging REST API endpoints to the A2rchi chat application for programmatic access to the ChromaDB vector database can be exposed with the following configuration change. To enable the ChromaDB endpoints, add the following to your config file under interfaces.chat_app : interfaces: chat_app: # ... other config options ... enable_debug_chroma_endpoints: true # Default: false ChromaDB Endpoints Info # /api/list_docs (GET) Lists all documents indexed in ChromaDB with pagination support. Query Parameters: - page : Page number (1-based, default: 1) - per_page : Documents per page (default: 50, max: 500) - content_length : Content preview length (default: -1 for full content) Response: { \"pagination\": { \"page\": 1, \"per_page\": 50, \"total_documents\": 1250, \"total_pages\": 25, \"has_next\": true, \"has_prev\": false }, \"documents\": [...] } # /api/search_docs (POST) Performs semantic search on the document collection using vector similarity. Request Body: - query : Search query string (required) - n_results : Number of results (default: 5, max: 100) - content_length : Max content length (default: -1, max: 5000) - include_full_content : Include complete document content (default: false) Response: { \"query\": \"machine learning\", \"search_params\": {...}, \"documents\": [ { \"content\": \"Document content...\", \"content_length\": 1200, \"metadata\": {...}, \"similarity_score\": 0.85 } ] } Vector Store TODO: explain vector store, retrievers, and related techniques Stemming By specifying the option stemming within ones configuration, stemming functionality for the documents in A2rchi will be enabled. By doing so, documents inserted into the ragging pipeline, as well as the query that is matched with them, will be stemmed and simplified for faster and more accurate lookup. utils: data_manager: stemming: ENABLED: true","title":"User Guide"},{"location":"user_guide/#user-guide","text":"","title":"User Guide"},{"location":"user_guide/#overview","text":"A2rchi supports various data sources as easy ways to ingest your data into the vector store databased used for document retrieval. These include: Links lists (even behind SSO) : automatically scrape and ingest documents from a list of URLs Git scraping : git mkdocs repositories Ticketing systems : JIRA, Redmine, Piazza Local documents Additionally, A2rchi supports various interfaces/services , which are applications that interact with the RAG system. These include: Chat interface : a web-based chat application Piazza integration : read posts from Piazza and post draft responses to a Slack channel Cleo/Redmine integration : read emails and create tickets in Redmine Mattermost integration : read posts from Mattermost and post draft responses to a Mattermost channel Grafana monitoring dashboard : monitor system and LLM performance metrics Document uploader : web interface for uploading and managing documents Grader : automated grading service for assignments with web interface Both data sources and interfaces/services are enabled via flags to the a2rchi create command, a2rchi create [...] --services=chatbot,piazza,jira,... The parameters of the services are configured via the configuration file. See below for more details. We support various pipelines which are pre-defined sequences of operations that process user inputs and generate responses. Each service may support a given pipeline. See the Services and Pipelines sections below for more details. For each pipeline, you can use different models, retrievers, and prompts for different steps of the pipeline. We support various models for both embeddings and LLMs, which can be run locally or accessed via APIs. See the Models section below for more details. Both pipelines and models are configured via the configuration file. Finally, we support various retrievers and embedding techniques for document retrieval. These are configured via the configuration file. See the Vector Store section below for more details.","title":"Overview"},{"location":"user_guide/#data-sources","text":"These are the different ways to ingest data into the vector store used for document retrieval.","title":"Data Sources"},{"location":"user_guide/#web-link-lists","text":"A web link list is a simple text file containing a list of URLs, one per line. A2rchi will fetch the content from each URL and add it to the vector store, using the Scraper class.","title":"WEb Link Lists"},{"location":"user_guide/#configuration","text":"You can define which lists of links A2rchi will inject in the configuration file as follows: data_manager: input_lists: # REQUIRED - configs/miscellanea.list # list of websites with relevant info - [...other lists...] Each list should be a simple text file containing one URL per line, e.g., https://example.com/page1 https://example.com/page2 [...] In the case that some of the links are behind a Single Sign-On (SSO) system, you can use the SSOScraper . To enable it, add the enable it and pick the class you want in the configuration file: utils: sso: sso_class: CERNSSO # or whichever class you want to use enabled: true","title":"Configuration"},{"location":"user_guide/#secrets","text":"If you are using SSO, depending on the class, you may need to provide your login credentials in a secrets file as follows: SSO_USERNAME=username SSO_PASSWORD=password Then, make sure that the links you provide in the .list file(s) start with sso- , e.g., sso-https://example.com/protected/page","title":"Secrets"},{"location":"user_guide/#running","text":"Link scraping is automatically enabled in A2rchi, you don't need to add any arguments to the create command.","title":"Running"},{"location":"user_guide/#git-scraping","text":"In some cases, the RAG input may be documentations based on MKDocs git repositories. Instead of scraping these sites as regular HTML sites you can obtain the relevant content using the GitScraper class.","title":"Git scraping"},{"location":"user_guide/#configuration_1","text":"To configure it, simply add the following field in the configuration file: utils: git: enabled: {{ utils.git.enabled | default(false, true) }} In the input lists, make sure to prepend git- to the URL of the repositories you are interested in scraping. git-https://gitlab.cern.ch/cms-tier0-ops/documentation.git","title":"Configuration"},{"location":"user_guide/#secrets_1","text":"You will need to provide a git username and token in the secrets file, GIT_USERNAME=your_username GIT_TOKEN=your_token","title":"Secrets"},{"location":"user_guide/#running_1","text":"Git link scraping is automatically enabled in A2rchi once enabled in the config, you don't need to add any arguments to the create command.","title":"Running"},{"location":"user_guide/#jira","text":"The JIRA integration allows A2rchi to fetch issues and comments from specified JIRA projects and add them to the vector store, using the JiraScraper class.","title":"JIRA"},{"location":"user_guide/#configuration_2","text":"Select which projects to scrape in the configuration file: jira: url: {{ utils.jira.JIRA_URL }} projects: {%- for project in utils.jira.JIRA_PROJECTS %} - {{ project }} {%- endfor %} anonymize_data: {{ utils.jira.ANONYMIZE_DATA | default(true, true) }} You can turn on an automatic anonymizer of the data fetched from JIRA via the anonymize_data config. anonymizer: nlp_model: {{ utils.anonymizer.nlp_model | default('en_core_web_sm', true) }} excluded_words: {%- for word in utils.anonymizer.excluded_words | default(['John', 'Jane', 'Doe']) %} - {{ word }} {%- endfor %} greeting_patterns: {%- for pattern in utils.anonymizer.greeting_patterns | default(['^(hi|hello|hey|greetings|dear)\\\\b', '^\\\\w+,\\\\s*']) %} - {{ pattern }} {%- endfor %} signoff_patterns: {%- for pattern in utils.anonymizer.signoff_patterns | default(['\\\\b(regards|sincerely|best regards|cheers|thank you)\\\\b', '^\\\\s*[-~]+\\\\s*$']) %} - {{ pattern }} {%- endfor %} email_pattern: '{{ utils.anonymizer.email_pattern | default(\"[\\\\w\\\\.-]+@[\\\\w\\\\.-]+\\\\.\\\\w+\") }}' username_pattern: '{{ utils.anonymizer.username_pattern | default(\"\\\\[~[^\\\\]]+\\\\]\") }}' The anonymizer will remove names, emails, usernames, greetings, signoffs, and any other words you specify from the fetched data. This is useful if you want to avoid having personal information in the vector store.","title":"Configuration"},{"location":"user_guide/#secrets_2","text":"A personal access token (PAT) is required to authenticate and authorize with JIRA. This token should be placed in a secrets file as JIRA_PAT .","title":"Secrets"},{"location":"user_guide/#running_2","text":"To enable JIRA scraping, run with, a2rchi create [...] --services=jira","title":"Running"},{"location":"user_guide/#adding-documents-and-the-uploader-interface","text":"","title":"Adding Documents and the Uploader Interface"},{"location":"user_guide/#adding-documents","text":"There are two main ways to add documents to A2rchi's vector database. They are: Manually adding files while the service is running via the uploader GUI Directly copying files into the container These methods are outlined below.","title":"Adding Documents"},{"location":"user_guide/#manual-uploader","text":"In order to upload documents while A2rchi is running via an easily accessible GUI, use the upload-manager built into the system. The manager is run as an additional docker service by adding the following argument to the CLI command: a2rchi create [...] --services=uploader The exact port may vary based on configuration (default is 5001 ). A simple docker ps -a command run on the server will inform which port it's being run on. In order to access the manager, one must first make an account. To do this, first get the ID or name of the uploader container using docker ps -a . Then, accese the container using docker exec -it <CONTAINER-ID> bash so you can run python bin/service_create_account.py from the /root/A2rchi/a2rchi directory.\u00b7 This script will guide you through creating an account. Note that we do not guarantee the security of this account, so never upload critical passwords to create it. Once you have created an account, visit the outgoing port of the data manager docker service and then log in. The GUI will then allow you to upload documents while A2rchi is still running. Note that it may take a few minutes for all the documents to upload.","title":"Manual Uploader"},{"location":"user_guide/#directly-copying-files-to-the-container","text":"The documents used for RAG live in the chat container at /root/data/<directory>/<files> . Thus, in a pinch, you can docker/podman cp a file at this directory level, e.g., podman/docker cp myfile.pdf <container name or ID>:/root/data/<new_dir>/ . If you need to make a new directory in the container, you can do podman exec -it <container name or ID> mkdir /root/data/<new_dir> .","title":"Directly copying files to the container"},{"location":"user_guide/#redmine","text":"Input from Redmine tickets as a data source.","title":"Redmine"},{"location":"user_guide/#secrets_3","text":"REDMINE_URL REDMINE_USER REDMINE_PW REDMINE_PROJECT","title":"Secrets"},{"location":"user_guide/#running_3","text":"a2rchi create [...] --services=redmine","title":"Running"},{"location":"user_guide/#interfacesservices","text":"These are the different apps that A2rchi supports, which allow you to interact with the AI pipelines.","title":"Interfaces/Services"},{"location":"user_guide/#piazza-interface","text":"Set up A2rchi to read posts from your Piazza forum and post draft responses to a specified Slack channel. To do this, a Piazza login (email and password) is required, plus the network ID of your Piazza channel, and lastly, a Webhook for the slack channel A2rchi will post to. See below for a step-by-step description of this. Go to https://api.slack.com/apps and sign in to workspace where you will eventually want A2rchi to post to (note doing this in a business workspace like the MIT one will require approval of the app/bot). Click 'Create New App', and then 'From scratch'. Name your app and again select the correct workspace. Then hit 'Create App' Now you have your app, and there are a few things to configure before you can launch A2rchi: Go to Incoming Webhooks under Features, and toggle it on. Click 'Add New Webhook', and select the channel you want A2rchi to post to. Now, copy the 'Webhook URL' and paste it into the secrets file, and handle it like any other secret!","title":"Piazza Interface"},{"location":"user_guide/#configuration_3","text":"Beyond standard required configuration fields, the network ID of the Piazza channel is required (see below for an example config). You can get the network ID by simply navigating to the class homepage, and grabbing the sequence that follows 'https://piazza.com/class/'. For example, the 8.01 Fall 2024 homepage is: 'https://piazza.com/class/m0g3v0ahsqm2lg'. The network ID is thus 'm0g3v0ahsqm2lg'. Example minimal config for the Piazza interface: name: bare_minimum_configuration #REQUIRED global: TRAINED_ON: \"Your class materials\" #REQUIRED chains: input_lists: #REQUIRED - configs/class_info.list # list of websites with class info a2rchi: [... a2rchi config ...] utils: piazza: network_id: <your Piazza network ID here> # REQUIRED","title":"Configuration"},{"location":"user_guide/#secrets_4","text":"The necessary secrets for deploying the Piazza service are the following: PIAZZA_EMAIL=... PIAZZA_PASSWORD=... SLACK_WEBHOOK=... The Slack webhook secret is described above. The Piazza email and password should be those of one of the class instructors. Remember to put this information in files named following what is written above.","title":"Secrets"},{"location":"user_guide/#running_4","text":"To run the Piazza service, simply add the piazza flag. For example: a2rchi create [...] --services=piazza","title":"Running"},{"location":"user_guide/#redminemailbox-interface","text":"A2rchi will read all new tickets in a Redmine project, and draft a response as a comment to the ticket. Once the ticket is updated to the \"Resolved\" status by an admin, A2rchi will send the response as an email to the user who opened the ticket. The admin can modify A2rchi's response before sending it out.","title":"Redmine/Mailbox Interface"},{"location":"user_guide/#configuration_4","text":"redmine: redmine_update_time: {{ utils.redmine.redmine_update_time | default(10, true) }} answer_tag: {{ utils.redmine.answer_tag | default('-- A2rchi -- Resolving email was sent', true) }}","title":"Configuration"},{"location":"user_guide/#secrets_5","text":"required_secrets=['IMAP_USER', 'IMAP_PW', 'REDMINE_URL', 'REDMINE_USER', 'REDMINE_PW', 'REDMINE_PROJECT', 'SENDER_SERVER', 'SENDER_PORT', 'SENDER_REPLYTO', 'SENDER_USER', 'SENDER_PW']","title":"Secrets"},{"location":"user_guide/#running_5","text":"a2rchi create [...] --services=redmine-mailer","title":"Running"},{"location":"user_guide/#mattermost-interface","text":"Set up A2rchi to read posts from your Mattermost forum and post draft responses to a specified Mattermost channel.","title":"Mattermost Interface"},{"location":"user_guide/#configuration_5","text":"mattermost: update_time: {{ utils.mattermost.update_time | default(60, true) }}","title":"Configuration"},{"location":"user_guide/#secrets_6","text":"You need to specify a webhook, a key and the id of two channels to read and write. Should be specified like this. MATTERMOST_WEBHOOK=... MATTERMOST_KEY=... MATTERMOST_CHANNEL_ID_READ=... MATTERMOST_CHANNEL_ID_WRITE=...","title":"Secrets"},{"location":"user_guide/#running_6","text":"To run the Mattermost service, simply add the Mattermost to the services flag. For example: a2rchi create [...] --services=mattermost","title":"Running"},{"location":"user_guide/#grafana-interface","text":"Monitor the performance of your A2rchi instance with the Grafana interface. This service provides a web-based dashboard to visualize various metrics related to system performance, LLM usage, and more. Note, if you are deploying a version of A2rchi you have already used (i.e., you haven't removed the images/volumes for a given --name ), the postgres will have already been created without the Grafana user created, and it will not work, so make sure to deploy a fresh instance.","title":"Grafana Interface"},{"location":"user_guide/#configuration_6","text":"grafana: external_port: {{ interfaces.grafana.external_port | default(3000, true) }}","title":"Configuration"},{"location":"user_guide/#secrets_7","text":"To run the Grafana service, you first need to specify a password for the Grafana to access the postgres database that stores the information. Set the environment variable as follows in the secrets file: PG_PASSWORD=<your_password>","title":"Secrets"},{"location":"user_guide/#running_7","text":"Once this is set, add the following argument to your a2rchi create command, e.g., a2rchi create [...] --services=grafana and you should see something like this CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES d27482864238 localhost/chromadb-gtesting2:2000 uvicorn chromadb.... 9 minutes ago Up 9 minutes (healthy) 0.0.0.0:8000->8000/tcp, 8000/tcp chromadb-gtesting2 87f1c7289d29 docker.io/library/postgres:16 postgres 9 minutes ago Up 9 minutes (healthy) 5432/tcp postgres-gtesting2 40130e8e23de docker.io/library/grafana-gtesting2:2000 9 minutes ago Up 9 minutes 0.0.0.0:3000->3000/tcp, 3000/tcp grafana-gtesting2 d6ce8a149439 localhost/chat-gtesting2:2000 python -u a2rchi/... 9 minutes ago Up 9 minutes 0.0.0.0:7861->7861/tcp chat-gtesting2 where the grafana interface is accessible at your-hostname:3000 . To change the external port from 3000 , you can do this in the config at interfaces:grafana:EXTERNAL_PORT . The default login and password are both \"admin\", which you will be prompted to change should you want to after first logging in. Navigate to the A2rchi dashboard from the home page by going to the menu > Dashboards > A2rchi > A2rchi Usage. Note, your-hostname here is the just name of the machine. Grafana uses its default configuration which is localhost but unlike the chat interface, there are no APIs where we template with a selected hostname, so the container networking handles this nicely. Pro tip: once at the web interface, for the \"Recent Conversation Messages (Clean Text + Link)\" panel, click the three little dots in the top right hand corner of the panel, click \"Edit\", and on the right, go to e.g., \"Override 4\" (should have Fields with name: clean text, also Override 7 for context column) and override property \"Cell options > Cell value inspect\". This will allow you to expand the text boxes with messages longer than can fit. Make sure you click apply to keep the changes. Pro tip 2: If you want to download all of the information from any panel as a CSV, go to the same three dots and click \"Inspect\", and you should see the option.","title":"Running"},{"location":"user_guide/#grader-interface","text":"Interface to launch a website which for a provided solution and rubric (and a couple of other things detailed below), will grade scanned images of a handwritten solution for the specified problem(s). Nota bene: this is not yet fully generalized and \"service\" ready, but instead for testing grading pipelines and a base off of which to build a potential grading app.","title":"Grader Interface"},{"location":"user_guide/#requirements","text":"To launch the service the following files are required: users.csv . This file is .csv file that contains two columns: \"MIT email\" and \"Unique code\", e.g.: MIT email,Unique code username@mit.edu,222 For now, the system requires the emails to be in the MIT domain, namely, contain \"@mit.edu\". TODO: make this an argument that is passed (e.g., school/email domain) solution_with_rubric_*.txt . These are .txt files that contain the problem solution followed by the rubric. The naming of the files should follow exactly, where the * is the problem number. There should be one of these files for every problem you want the app to be able to grade. The top of the file should be the problem name with a line of dashes (\"-\") below, e.g.: Anti-Helmholtz Coils --------------------------------------------------- These files should live in a directory which you will pass to the config, and A2rchi will handle the rest. admin_password.txt . This file will be passed as a secret and be the admin code to login in to the page where you can reset attempts for students.","title":"Requirements"},{"location":"user_guide/#secrets_8","text":"The only grading specific secret is the admin password, which like shown above, should be put in the following file ADMIN_PASSWORD=your_password Then it behaves like any other secret.","title":"Secrets"},{"location":"user_guide/#configuration_7","text":"The required fields in the configuration file are different from the rest of the A2rchi services. Below is an example: name: grading_test # REQUIRED global: TRAINED_ON: \"rubrics, class info, etc.\" # REQUIRED a2rchi: pipelines: - GradingPipeline pipeline_map: GradingPipeline: prompts: required: final_grade_prompt: configs/prompts/final_grade.prompt models: required: final_grade_model: OllamaInterface ImageProcessingPipeline: prompts: required: image_processing_prompt: configs/prompts/image_processing.prompt models: required: image_processing_model: OllamaInterface interfaces: grader_app: num_problems: 1 # REQUIRED local_rubric_dir: ~/grading/my_rubrics # REQUIRED local_users_csv_dir: ~/grading/logins # REQUIRED data_manager: [...] name -- The name of your configuration (required). global.TRAINED_ON -- A brief description of the data or materials A2rchi is trained on (required). a2rchi.pipelines -- List of pipelines to use (e.g., GradingPipeline , ImageProcessingPipeline ). a2rchi.pipeline_map -- Mapping of pipelines to their required prompts and models. a2rchi.pipeline_map.GradingPipeline.prompts.required.final_grade_prompt -- Path to the grading prompt file for evaluating student solutions. a2rchi.pipeline_map.GradingPipeline.models.required.final_grade_model -- Model class for grading (e.g., OllamaInterface , HuggingFaceOpenLLM ). a2rchi.pipeline_map.ImageProcessingPipeline.prompts.required.image_processing_prompt -- Path to the prompt file for image processing. a2rchi.pipeline_map.ImageProcessingPipeline.models.required.image_processing_model -- Model class for image processing (e.g., OllamaInterface , HuggingFaceImageLLM ). interfaces.grader_app.num_problems -- Number of problems the grading service should expect (must match the number of rubric files). interfaces.grader_app.local_rubric_dir -- Directory containing the solution_with_rubric_*.txt files. interfaces.grader_app.local_users_csv_dir -- Directory containing the users.csv file.","title":"Configuration"},{"location":"user_guide/#running_8","text":"a2rchi create [...] --services=grader","title":"Running"},{"location":"user_guide/#models","text":"Models are either: Hosted locally, either via VLLM or HuggingFace transformers. Accessed via an API, e.g., OpenAI, Anthropic, etc. Accessed via an Ollama server instance.","title":"Models"},{"location":"user_guide/#local-models","text":"To use a local model, specify one of the local model classes in models.py : HuggingFaceOpenLLM HuggingFaceImageLLM VLLM","title":"Local Models"},{"location":"user_guide/#models-via-apis","text":"We support the following model classes in models.py for models accessed via APIs: OpenAILLM ClaudeLLM AnthropicLLM","title":"Models via APIs"},{"location":"user_guide/#ollama","text":"In order to use an Ollama server instance for the chatbot, it is possible to specify OllamaInterface for the model name. To then correctly use models on the Ollama server, in the keyword args, specify both the url of the server and the name of a model hosted on the server. a2rchi: chain: model_class_map: OllamaInterface: kwargs: base_model: \"gemma3\" # example url: \"url-for-server\" In this case, the gemma3 model is hosted on the Ollama server at url-for-server . You can check which models are hosted on your server by going to url-for-server/models .","title":"Ollama"},{"location":"user_guide/#other","text":"Some useful additional features supported by the framework.","title":"Other"},{"location":"user_guide/#add-chromadb-document-management-api-endpoints","text":"","title":"Add ChromaDB Document Management API Endpoints"},{"location":"user_guide/#debugging-chromadb-endpoints","text":"Debugging REST API endpoints to the A2rchi chat application for programmatic access to the ChromaDB vector database can be exposed with the following configuration change. To enable the ChromaDB endpoints, add the following to your config file under interfaces.chat_app : interfaces: chat_app: # ... other config options ... enable_debug_chroma_endpoints: true # Default: false","title":"Debugging ChromaDB endpoints"},{"location":"user_guide/#chromadb-endpoints-info","text":"","title":"ChromaDB Endpoints Info"},{"location":"user_guide/#apilist_docs-get","text":"Lists all documents indexed in ChromaDB with pagination support. Query Parameters: - page : Page number (1-based, default: 1) - per_page : Documents per page (default: 50, max: 500) - content_length : Content preview length (default: -1 for full content) Response: { \"pagination\": { \"page\": 1, \"per_page\": 50, \"total_documents\": 1250, \"total_pages\": 25, \"has_next\": true, \"has_prev\": false }, \"documents\": [...] }","title":"# /api/list_docs (GET)"},{"location":"user_guide/#apisearch_docs-post","text":"Performs semantic search on the document collection using vector similarity. Request Body: - query : Search query string (required) - n_results : Number of results (default: 5, max: 100) - content_length : Max content length (default: -1, max: 5000) - include_full_content : Include complete document content (default: false) Response: { \"query\": \"machine learning\", \"search_params\": {...}, \"documents\": [ { \"content\": \"Document content...\", \"content_length\": 1200, \"metadata\": {...}, \"similarity_score\": 0.85 } ] }","title":"# /api/search_docs (POST)"},{"location":"user_guide/#vector-store","text":"TODO: explain vector store, retrievers, and related techniques","title":"Vector Store"},{"location":"user_guide/#stemming","text":"By specifying the option stemming within ones configuration, stemming functionality for the documents in A2rchi will be enabled. By doing so, documents inserted into the ragging pipeline, as well as the query that is matched with them, will be stemmed and simplified for faster and more accurate lookup. utils: data_manager: stemming: ENABLED: true","title":"Stemming"}]}